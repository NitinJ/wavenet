{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d89768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch, os, bisect, json\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchaudio\")\n",
    "\n",
    "# Add this at the beginning of your notebook:\n",
    "import torch._inductor.config as config\n",
    "config.triton.cudagraphs = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    major, minor = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {major}.{minor}\")\n",
    "    if major >= 8:\n",
    "        print(\"✅ TF32 is supported (Ampere or newer).\")\n",
    "    else:\n",
    "        print(\"❌ TF32 is not supported.\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'  # For better debugging\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79332a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Load LJSpeech dataset.\n",
    "# Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22050 Hz.\n",
    "dataset = torchaudio.datasets.LJSPEECH(\n",
    "    root=\"./data\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Print dataset info.\n",
    "print(f\"Number of samples in dataset: {len(dataset)}\")\n",
    "print(f\"Sample rate: {dataset[0][1]}\")\n",
    "print(f\"Example utterance text: {dataset[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuLawEncoding:\n",
    "    def __init__(self, quantization_channels: int = 256):\n",
    "        self.Q = quantization_channels\n",
    "        self.enc = torchaudio.transforms.MuLawEncoding(self.Q)\n",
    "        self.dec = torchaudio.transforms.MuLawDecoding(self.Q)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mu_law_encode(self, x: torch.Tensor) -> torch.LongTensor:\n",
    "        # x: [..., T] float in [-1, 1]\n",
    "        return self.enc(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mu_law_decode(self, q: torch.Tensor) -> torch.Tensor:\n",
    "        # q: [..., T] long/int in [0, Q-1]\n",
    "        return self.dec(q)\n",
    "    \n",
    "# Save codec once and use everywhere.\n",
    "codec = MuLawEncoding(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 2,  # CHANGED: Use 0 for debugging, multiprocessing can cause issues\n",
    "    \"pin_memory\": True,\n",
    "    \"mu\": 256,\n",
    "    \"sr\": 16000,\n",
    "    \"trim_silence_thresh\": 1e-3,\n",
    "    \"window_size\": 32001,  # ~2 seconds at 16kHz\n",
    "\n",
    "    # Wavenet architecture related.\n",
    "    \"residual_channels\": 64,\n",
    "    \"skip_channels\": 256,\n",
    "    \"output_dim\": 256,\n",
    "    \"n_layers\": 10,\n",
    "    \"n_blocks\": 5,\n",
    "    \"kernel_size\": 2,\n",
    "    'hop_size': 16000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self):\n",
    "        self.mu_law_encoding = codec\n",
    "        self.resamplers = {}  # Cache resamplers for efficiency\n",
    "\n",
    "    def normalize(self, x):\n",
    "        # Ensure audio is float32 and normalize to [-1, 1]\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        \n",
    "        # peak normalization\n",
    "        max_val = torch.max(torch.abs(x))\n",
    "        if max_val > 0:  # Avoid division by zero\n",
    "            x = x / max_val\n",
    "        \n",
    "        # rms normalization (optional)\n",
    "        target_rms = 0.1\n",
    "        def rms(x):\n",
    "            return torch.sqrt(torch.mean(x**2) + 1e-8)\n",
    "        current_rms = rms(x)\n",
    "        if current_rms > 0:  # Avoid division by zero\n",
    "            x = x * (target_rms / current_rms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def resample_audio(self, audio, orig_sr, target_sr):\n",
    "        if orig_sr == target_sr:\n",
    "            return audio\n",
    "            \n",
    "        # Use cached resampler for efficiency\n",
    "        resampler_key = f\"{orig_sr}_{target_sr}\"\n",
    "        if resampler_key not in self.resamplers:\n",
    "            self.resamplers[resampler_key] = torchaudio.transforms.Resample(\n",
    "                orig_freq=orig_sr,\n",
    "                new_freq=target_sr\n",
    "            )\n",
    "        \n",
    "        return self.resamplers[resampler_key](audio)\n",
    "\n",
    "    # Trim leading/trailing silence\n",
    "    def trim_silence(self, sig, thresh=config['trim_silence_thresh']):\n",
    "        # sig: [1,T] tensor\n",
    "        # returns: [1,T'] tensor with silence trimmed\n",
    "        # Calculate energy\n",
    "        energy = sig.abs().squeeze()\n",
    "        # Find indices where energy is above threshold\n",
    "        idx = torch.where(energy > thresh)[0]\n",
    "        if len(idx) == 0:\n",
    "            return sig  # Return original if no samples above threshold\n",
    "        # Return trimmed signal\n",
    "        return sig[:, idx[0].item():idx[-1].item() + 1]\n",
    "\n",
    "    def segment_audio(self, audio, drop_last=True, hop_size=None):\n",
    "        \"\"\"\n",
    "        Split [1, T] into windows of size config['window_size'].\n",
    "        - hop_size=None → non-overlapping\n",
    "        - hop_size < window_size → overlapping\n",
    "        - drop_last=True → drop incomplete tail (recommended to avoid padding loss)\n",
    "        \"\"\"\n",
    "        window_size = config['window_size']\n",
    "        hop = hop_size or window_size\n",
    "        T = audio.shape[1]\n",
    "\n",
    "        segments = []\n",
    "        # all full windows\n",
    "        for start in range(0, T - window_size + 1, hop):\n",
    "            segments.append(audio[:, start:start + window_size])\n",
    "\n",
    "        # optional last (padded) window\n",
    "        if not drop_last and (T < window_size or (T - window_size) % hop != 0):\n",
    "            last_start = max(0, T - window_size)\n",
    "            tail = audio[:, last_start:]\n",
    "            if tail.shape[1] < window_size:\n",
    "                tail = F.pad(tail, (0, window_size - tail.shape[1]))\n",
    "            segments.append(tail)\n",
    "\n",
    "        return segments\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x: LongTensor [B_total, W-1]\n",
    "            y: LongTensor [B_total, W-1]\n",
    "        where B_total = sum of (#segments per item in batch).\n",
    "        \"\"\"\n",
    "        x_list, y_list = [], []\n",
    "\n",
    "        for (audio, sr, text, normalized_text) in batch:\n",
    "            # resample → (optional) trim → normalize\n",
    "            audio = self.resample_audio(audio, sr, config['sr'])\n",
    "            audio = self.trim_silence(audio)              # you can disable if you want\n",
    "            audio = self.normalize(audio)                 # peak-normalize only\n",
    "\n",
    "            if audio.size(0) > 1:                         # safety: downmix stereo\n",
    "                audio = audio.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Generate multiple segments\n",
    "            segments = self.segment_audio(\n",
    "                audio,\n",
    "                drop_last=True,                           # avoid padded tails in loss\n",
    "                hop_size=config.get('hop_size', 16000)     # e.g., 16000 for 50% overlap @32000\n",
    "            )\n",
    "\n",
    "            # convert each segment to tokens and teacher-forced pairs\n",
    "            for seg in segments:\n",
    "                q = self.mu_law_encoding.mu_law_encode(seg.squeeze(0))  # [W]\n",
    "                x_list.append(q[:-1])\n",
    "                y_list.append(q[1:])\n",
    "\n",
    "        if len(x_list) == 0:\n",
    "            return (torch.empty(0, 0, dtype=torch.long),\n",
    "                    torch.empty(0, 0, dtype=torch.long))\n",
    "\n",
    "        max_len = max(t.size(0) for t in x_list)\n",
    "        x = torch.stack([F.pad(t, (0, max_len - t.size(0)), value=0) for t in x_list], dim=0)\n",
    "        y = torch.stack([F.pad(t, (0, max_len - t.size(0)), value=0) for t in y_list], dim=0)\n",
    "        return x.long(), y.long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef694d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674698ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmented_tokens_io.py\n",
    "\n",
    "import os, json, bisect\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# REQUIREMENT: You already defined AudioProcessor elsewhere.\n",
    "# It must expose: collate_fn([dataset[i]]) -> (x_segments, y_segments)\n",
    "# where each is Long tensor [Si, T] (Si = #segments from item i).\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ---------- Builder (parallel, RAM; fast if dataset fits in RAM) ----------\n",
    "\n",
    "class _SegmentsPerItem(Dataset):\n",
    "    \"\"\"Wraps a base dataset so each __getitem__ returns all (x,y) segments for that item.\"\"\"\n",
    "    def __init__(self, base_ds, audio_processor):\n",
    "        self.ds = base_ds\n",
    "        self.proc = audio_processor\n",
    "\n",
    "    def __len__(self): return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.proc.collate_fn([self.ds[i]])  # x:[Si,T], y:[Si,T] or empty\n",
    "        return x, y\n",
    "\n",
    "def _cat_collate(batch):\n",
    "    xs = [b[0] for b in batch if b[0].numel() > 0]\n",
    "    ys = [b[1] for b in batch if b[1].numel() > 0]\n",
    "    if not xs: return None\n",
    "    return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "\n",
    "def build_segmented_dataset_fast(base_dataset,\n",
    "                                 audio_processor,\n",
    "                                 num_workers: int = 8,\n",
    "                                 batch_items: int = 32) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      audio_x: [N, T] Long\n",
    "      audio_y: [N, T] Long\n",
    "    \"\"\"\n",
    "    wrapped = _SegmentsPerItem(base_dataset, audio_processor)\n",
    "    loader = DataLoader(\n",
    "        wrapped,\n",
    "        batch_size=batch_items,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        pin_memory=False,\n",
    "        prefetch_factor=(4 if num_workers > 0 else None),\n",
    "        collate_fn=_cat_collate,\n",
    "    )\n",
    "    xs, ys = [], []\n",
    "    for out in tqdm(loader, total=(len(base_dataset)+batch_items-1)//batch_items, desc=\"Pre-segmenting\"):\n",
    "        if out is None: continue\n",
    "        x, y = out\n",
    "        xs.append(x.contiguous())\n",
    "        ys.append(y.contiguous())\n",
    "    if not xs:\n",
    "        return (torch.empty(0, 0, dtype=torch.long), torch.empty(0, 0, dtype=torch.long))\n",
    "    audio_x = torch.cat(xs, dim=0)\n",
    "    audio_y = torch.cat(ys, dim=0)\n",
    "    return audio_x, audio_y\n",
    "\n",
    "# ---------- Saver (sharded) & Manifest ----------\n",
    "\n",
    "def save_segmented_dataset_sharded(audio_x: torch.Tensor,\n",
    "                                         audio_y: torch.Tensor,\n",
    "                                         out_dir: str,\n",
    "                                         shard_size: int = 512):\n",
    "    \"\"\"\n",
    "    Saves shards as uint8 to cut memory/disk by 8×.\n",
    "    audio_x/audio_y are Long in [0,255]; we store as uint8 and upcast on load.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import json, torch\n",
    "\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    N, T = audio_x.size(0), audio_x.size(1)\n",
    "    assert audio_y.size(0) == N and audio_y.size(1) == T\n",
    "\n",
    "    shard_files, shard_sizes = [], []\n",
    "    for s in range(0, N, shard_size):\n",
    "        e = min(s + shard_size, N)\n",
    "        x_u8 = audio_x[s:e].to(torch.uint8).contiguous()\n",
    "        y_u8 = audio_y[s:e].to(torch.uint8).contiguous()\n",
    "        f = f\"shard_{s//shard_size:05d}.pt\"\n",
    "        torch.save({\"x_u8\": x_u8, \"y_u8\": y_u8}, out / f)\n",
    "        shard_files.append(f)\n",
    "        shard_sizes.append(e - s)\n",
    "\n",
    "    manifest = {\n",
    "        \"version\": 2,\n",
    "        \"num_samples\": N,\n",
    "        \"seq_len\": T,\n",
    "        \"stored_dtype\": \"uint8\",\n",
    "        \"target_dtype\": \"long\",     # what the model expects\n",
    "        \"shard_size\": shard_size,\n",
    "        \"shards\": shard_files,\n",
    "        \"shard_sizes\": shard_sizes,\n",
    "    }\n",
    "    (out / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "    return out / \"manifest.json\"\n",
    "\n",
    "\n",
    "# ---------- Streaming writer (low-RAM; writes shards on the fly) ----------\n",
    "\n",
    "def stream_preprocess_to_shards(base_dataset,\n",
    "                                audio_processor,\n",
    "                                out_dir: str,\n",
    "                                shard_size: int = 10000,\n",
    "                                num_workers: int = 8,\n",
    "                                batch_items: int = 32):\n",
    "    \"\"\"\n",
    "    Builds (x,y) token segments in parallel but writes shards incrementally to keep RAM low.\n",
    "    Each shard has exactly <= shard_size samples. Handles partial consumption of a batch.\n",
    "    Returns path to manifest.json.\n",
    "    \"\"\"\n",
    "    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- dataset wrappers (same as before) ---\n",
    "    class _SegmentsPerItem(torch.utils.data.Dataset):\n",
    "        def __init__(self, base_ds, proc):\n",
    "            self.ds = base_ds\n",
    "            self.proc = proc\n",
    "        def __len__(self): return len(self.ds)\n",
    "        def __getitem__(self, i):\n",
    "            return self.proc.collate_fn([self.ds[i]])  # (x:[Si,T], y:[Si,T])\n",
    "\n",
    "    def _cat_collate(batch):\n",
    "        xs = [b[0] for b in batch if b[0].numel() > 0]\n",
    "        ys = [b[1] for b in batch if b[1].numel() > 0]\n",
    "        if not xs:\n",
    "            return None\n",
    "        x = torch.cat(xs, dim=0).contiguous()\n",
    "        y = torch.cat(ys, dim=0).contiguous()\n",
    "        return x, y\n",
    "\n",
    "    wrapped = _SegmentsPerItem(base_dataset, audio_processor)\n",
    "    loader = DataLoader(\n",
    "        wrapped,\n",
    "        batch_size=batch_items,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        pin_memory=False,\n",
    "        prefetch_factor=(4 if num_workers > 0 else None),\n",
    "        collate_fn=_cat_collate,\n",
    "    )\n",
    "\n",
    "    # Buffers of tensors to consume from (FIFO)\n",
    "    buf_x, buf_y = deque(), deque()\n",
    "    buf_count = 0                       # total samples in buffers\n",
    "    shard_idx = 0\n",
    "    shard_files, shard_sizes = [], []\n",
    "    total = 0\n",
    "    seq_len = None\n",
    "    dtype = None\n",
    "\n",
    "    def _push_batch(x, y):\n",
    "        nonlocal buf_count, seq_len, dtype\n",
    "        if x is None: return\n",
    "        assert x.size(0) == y.size(0)\n",
    "        if seq_len is None:\n",
    "            seq_len = x.size(1)\n",
    "            dtype = str(x.dtype).replace(\"torch.\", \"\")\n",
    "        else:\n",
    "            # sanity: all segments must share same length\n",
    "            assert x.size(1) == seq_len, \"Inconsistent segment length T across batches\"\n",
    "        buf_x.append(x)\n",
    "        buf_y.append(y)\n",
    "        buf_count += x.size(0)\n",
    "\n",
    "    def _pop_exact_n(n):\n",
    "        \"\"\"Pop exactly n samples from buffers, returning cat(x_parts), cat(y_parts).\n",
    "           Supports splitting the front tensor if needed.\n",
    "        \"\"\"\n",
    "        nonlocal buf_count\n",
    "        parts_x, parts_y = [], []\n",
    "        need = n\n",
    "        while need > 0:\n",
    "            assert buf_x, \"Buffer underflow\"\n",
    "            x0, y0 = buf_x[0], buf_y[0]\n",
    "            m = x0.size(0)\n",
    "            if m <= need:\n",
    "                # take whole front tensor\n",
    "                parts_x.append(x0)\n",
    "                parts_y.append(y0)\n",
    "                buf_x.popleft(); buf_y.popleft()\n",
    "                buf_count -= m\n",
    "                need -= m\n",
    "            else:\n",
    "                # take a slice and put leftovers back\n",
    "                parts_x.append(x0[:need])\n",
    "                parts_y.append(y0[:need])\n",
    "                buf_x[0] = x0[need:]\n",
    "                buf_y[0] = y0[need:]\n",
    "                buf_count -= need\n",
    "                need = 0\n",
    "        X = torch.cat(parts_x, dim=0)\n",
    "        Y = torch.cat(parts_y, dim=0)\n",
    "        return X, Y\n",
    "\n",
    "    def _flush_if_ready(force=False):\n",
    "        \"\"\"Write shards while we have >= shard_size, or if force=True write remaining.\"\"\"\n",
    "        nonlocal shard_idx, total\n",
    "        while buf_count >= shard_size or (force and buf_count > 0):\n",
    "            take = shard_size if buf_count >= shard_size else buf_count\n",
    "            X, Y = _pop_exact_n(take)\n",
    "            f = f\"shard_{shard_idx:05d}.pt\"\n",
    "            torch.save({\"x\": X, \"y\": Y}, out / f)\n",
    "            shard_files.append(f)\n",
    "            shard_sizes.append(X.size(0))\n",
    "            total += X.size(0)\n",
    "            shard_idx += 1\n",
    "\n",
    "    # main loop\n",
    "    for out_batch in tqdm(loader, total=(len(base_dataset) + batch_items - 1)//batch_items, desc=\"Streaming pre-seg\"):\n",
    "        if out_batch is None:\n",
    "            continue\n",
    "        x, y = out_batch\n",
    "        _push_batch(x, y)\n",
    "        _flush_if_ready(force=False)\n",
    "\n",
    "    # final flush\n",
    "    _flush_if_ready(force=True)\n",
    "\n",
    "    # write manifest\n",
    "    manifest = {\n",
    "        \"version\": 1,\n",
    "        \"num_samples\": total,\n",
    "        \"seq_len\": seq_len if seq_len is not None else 0,\n",
    "        \"dtype\": dtype if dtype is not None else \"long\",\n",
    "        \"shard_size\": shard_size,\n",
    "        \"shards\": shard_files,\n",
    "        \"shard_sizes\": shard_sizes,\n",
    "    }\n",
    "    (out / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "    return out / \"manifest.json\"\n",
    "# ---------- Loader (direct use in training) ----------\n",
    "\n",
    "class SegmentedTokensOnDisk(torch.utils.data.Dataset):\n",
    "    def __init__(self, manifest_path, root=None, cache_shards=False):\n",
    "        import json, torch\n",
    "        from pathlib import Path\n",
    "        mp = Path(manifest_path)\n",
    "        man = json.loads(mp.read_text())\n",
    "        self.root = mp.parent if root is None else Path(root)\n",
    "        self.files = [self.root / s for s in man[\"shards\"]]\n",
    "        self.sizes = man[\"shard_sizes\"]\n",
    "        self.cum = []\n",
    "        c = 0\n",
    "        for s in self.sizes:\n",
    "            c += s\n",
    "            self.cum.append(c)\n",
    "        self.N = man[\"num_samples\"]\n",
    "        self.T = man[\"seq_len\"]\n",
    "        self.cache = {} if cache_shards else None\n",
    "        self.stored_dtype = man.get(\"stored_dtype\", \"long\")\n",
    "\n",
    "    def __len__(self): return self.N\n",
    "\n",
    "    def _loc(self, idx):\n",
    "        import bisect\n",
    "        s_idx = bisect.bisect_right(self.cum, idx)\n",
    "        base = 0 if s_idx == 0 else self.cum[s_idx-1]\n",
    "        off = idx - base\n",
    "        return s_idx, off\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s_idx, off = self._loc(idx)\n",
    "        if self.cache is not None and s_idx in self.cache:\n",
    "            shard = self.cache[s_idx]\n",
    "        else:\n",
    "            shard = torch.load(self.files[s_idx], map_location=\"cpu\")\n",
    "            if self.cache is not None:\n",
    "                self.cache[s_idx] = shard\n",
    "\n",
    "        if self.stored_dtype == \"uint8\":\n",
    "            x = shard[\"x_u8\"][off].to(torch.long)  # upcast once\n",
    "            y = shard[\"y_u8\"][off].to(torch.long)\n",
    "        else:\n",
    "            x = shard[\"x\"][off]\n",
    "            y = shard[\"y\"][off]\n",
    "        return x, y\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AudioProcessor()\n",
    "out_dir = \"./segmented_tokens\"          # where to save shards\n",
    "shard_size = 10000\n",
    "\n",
    "manifest = stream_preprocess_to_shards(\n",
    "    base_dataset=dataset,\n",
    "    audio_processor=ap,\n",
    "    out_dir=out_dir,\n",
    "    shard_size=shard_size,\n",
    "    num_workers=8,\n",
    "    batch_items=32,\n",
    ")\n",
    "print(\"Manifest written to:\", manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load later for training:\n",
    "ds = SegmentedTokensOnDisk(\"segmented_tokens/manifest.json\", cache_shards=False)\n",
    "\n",
    "# Start conservative:\n",
    "dataset_loader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,          # first run; ensure it works\n",
    "    pin_memory=False,       # you can flip to True later if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset_loader:\n",
    "    print(f\"x: {batch[0].shape}, y: {batch[1].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b29d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_for_oob(manifest_path):\n",
    "    mp = Path(manifest_path)\n",
    "    man = json.loads(mp.read_text())\n",
    "    root = mp.parent\n",
    "    bads = []\n",
    "\n",
    "    for sid, fname in enumerate(man[\"shards\"]):\n",
    "        obj = torch.load(root / fname, map_location=\"cpu\")\n",
    "        # support both uint8-v2 and long-v1\n",
    "        if \"x_u8\" in obj:\n",
    "            X = obj[\"x_u8\"].to(torch.int16)  # cheap\n",
    "            Y = obj[\"y_u8\"].to(torch.int16)\n",
    "        else:\n",
    "            X = obj[\"x\"]; Y = obj[\"y\"]\n",
    "\n",
    "        bad_x = (X < 0) | (X > 255)\n",
    "        bad_y = (Y < 0) | (Y > 255)\n",
    "        if bad_x.any() or bad_y.any():\n",
    "            ix = bad_x.nonzero(as_tuple=False)\n",
    "            iy = bad_y.nonzero(as_tuple=False)\n",
    "            print(f\"[shard {sid}] bad X: {ix.shape[0]}, bad Y: {iy.shape[0]}\")\n",
    "            # record first few\n",
    "            for n in ix[:5]:\n",
    "                r,c = n.tolist()\n",
    "                print(\"  X oob at sample\", r, \"pos\", c, \"val\", int(X[r,c]))\n",
    "                bads.append((\"x\", sid, r, c, int(X[r,c])))\n",
    "            for n in iy[:5]:\n",
    "                r,c = n.tolist()\n",
    "                print(\"  Y oob at sample\", r, \"pos\", c, \"val\", int(Y[r,c]))\n",
    "                bads.append((\"y\", sid, r, c, int(Y[r,c])))\n",
    "    return bads\n",
    "\n",
    "bads = scan_for_oob(\"segmented_tokens/manifest.json\")\n",
    "print(\"Total OOB entries:\", len(bads))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
