{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61003f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._inductor.config as config\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchaudio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    major, minor = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute Capability: {major}.{minor}\")\n",
    "    if major >= 8:\n",
    "        print(\"✅ TF32 is supported (Ampere or newer).\")\n",
    "    else:\n",
    "        print(\"❌ TF32 is not supported.\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'  # For better debugging\n",
    "\n",
    "config.triton.cudagraphs = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a491c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentedTokensOnDisk(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for loading audio tokens stored in sharded files on disk.\n",
    "    \n",
    "    This dataset efficiently loads pre-tokenized audio data from multiple shard files,\n",
    "    enabling training on large audio datasets that don't fit in memory. Each sample\n",
    "    consists of input tokens (x) and target tokens (y) for autoregressive training.\n",
    "    \n",
    "    Args:\n",
    "        manifest_path (str): Path to JSON manifest file containing shard information\n",
    "        root (str, optional): Root directory for shard files. Defaults to manifest directory\n",
    "        cache_shards (bool): Whether to cache loaded shards in memory. Defaults to True\n",
    "        \n",
    "    Manifest JSON format:\n",
    "        {\n",
    "            \"shards\": [\"shard_0.pt\", \"shard_1.pt\", ...],\n",
    "            \"shard_sizes\": [1000, 1000, ...],\n",
    "            \"num_samples\": 2000,\n",
    "            \"seq_len\": 32000,\n",
    "            \"stored_dtype\": \"long\" or \"uint8\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, manifest_path, root=None, cache_shards=True):\n",
    "        import json\n",
    "        import torch\n",
    "        from pathlib import Path\n",
    "        mp = Path(manifest_path)\n",
    "        man = json.loads(mp.read_text())\n",
    "        self.root = mp.parent if root is None else Path(root)\n",
    "        self.files = [self.root / s for s in man[\"shards\"]]\n",
    "        self.sizes = man[\"shard_sizes\"]  # Number of samples per shard\n",
    "        \n",
    "        # Compute cumulative sizes for efficient sample location\n",
    "        self.cum = []\n",
    "        c = 0\n",
    "        for s in self.sizes:\n",
    "            c += s\n",
    "            self.cum.append(c)\n",
    "            \n",
    "        self.N = man[\"num_samples\"]  # Total number of samples\n",
    "        self.T = man[\"seq_len\"]      # Sequence length per sample\n",
    "        self.cache = {}              # Cache for loaded shards\n",
    "        self.stored_dtype = man.get(\"stored_dtype\", \"long\")\n",
    "        self.current_shard = -1      # Track currently loaded shard\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total number of samples across all shards\n",
    "        \"\"\"\n",
    "        return self.N\n",
    "\n",
    "    def _loc(self, idx):\n",
    "        \"\"\"\n",
    "        Locate which shard contains the sample at given index and its offset within that shard.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Global sample index\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (shard_index, offset_within_shard)\n",
    "                - shard_index (int): Index of the shard containing the sample\n",
    "                - offset_within_shard (int): Position of sample within the shard\n",
    "        \"\"\"\n",
    "        import bisect\n",
    "        s_idx = bisect.bisect_right(self.cum, idx)\n",
    "        base = 0 if s_idx == 0 else self.cum[s_idx-1]\n",
    "        off = idx - base\n",
    "        return s_idx, off\n",
    "\n",
    "    def get_shard(self, s_idx):\n",
    "        \"\"\"\n",
    "        Load and cache a shard file containing tokenized audio data.\n",
    "        \n",
    "        Args:\n",
    "            s_idx (int): Index of the shard to load\n",
    "            \n",
    "        Returns:\n",
    "            dict: Loaded shard data containing:\n",
    "                - 'x' or 'x_u8': Input token sequences, shape [shard_size, seq_len]\n",
    "                - 'y' or 'y_u8': Target token sequences, shape [shard_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Clear cache if switching to different shard to save memory\n",
    "        if self.current_shard != s_idx:\n",
    "            self.cache = {}\n",
    "            self.current_shard = -1\n",
    "\n",
    "        self.current_shard = s_idx\n",
    "\n",
    "        if s_idx in self.cache:\n",
    "            shard = self.cache[s_idx]\n",
    "        else:\n",
    "            print(f\"Loading shard {s_idx} from {self.files[s_idx]}\")\n",
    "            shard = torch.load(self.files[s_idx], map_location=\"cpu\")\n",
    "            self.cache[s_idx] = shard\n",
    "        return shard\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample (input, target) pair for autoregressive training.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Sample index (0 to len(dataset)-1)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (x, y) where:\n",
    "                - x: Input tokens, shape [seq_len], dtype torch.long, values in [0, 255]\n",
    "                - y: Target tokens, shape [seq_len], dtype torch.long, values in [0, 255]\n",
    "                \n",
    "        Note:\n",
    "            Token values are clamped to [0, 255] range to ensure valid mu-law encoding.\n",
    "        \"\"\"\n",
    "        s_idx, off = self._loc(idx)\n",
    "        shard = self.get_shard(s_idx)\n",
    "        \n",
    "        if self.stored_dtype == \"uint8\":\n",
    "            x = shard[\"x_u8\"][off].to(torch.long)  # upcast once\n",
    "            y = shard[\"y_u8\"][off].to(torch.long)\n",
    "        else:\n",
    "            x = shard[\"x\"][off]\n",
    "            y = shard[\"y\"][off]\n",
    "            \n",
    "        # Ensure tokens are in valid range for mu-law encoding\n",
    "        if (x.gt(255).any() or y.gt(255).any()) or (x.lt(0).any() or y.lt(0).any()):\n",
    "            x.clamp_(0, 255)\n",
    "            y.clamp_(0, 255)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "dataset = SegmentedTokensOnDisk(\n",
    "    \"segmented_tokens/manifest.json\", cache_shards=False)\n",
    "\n",
    "# Print dataset info.\n",
    "print(f\"Number of samples in dataset: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuLawEncoding:\n",
    "    \"\"\"\n",
    "    Mu-law encoding and decoding for audio signal compression and quantization.\n",
    "    \n",
    "    Mu-law encoding compresses the dynamic range of audio signals by applying\n",
    "    logarithmic quantization, which provides better perceptual quality for speech\n",
    "    and audio. This is commonly used in telephony and audio codecs.\n",
    "    \n",
    "    Args:\n",
    "        quantization_channels (int): Number of discrete quantization levels.\n",
    "                                   Default is 256 (8-bit quantization).\n",
    "    \n",
    "    Attributes:\n",
    "        Q (int): Number of quantization channels\n",
    "        enc (torchaudio.transforms.MuLawEncoding): Encoder transform\n",
    "        dec (torchaudio.transforms.MuLawDecoding): Decoder transform\n",
    "    \"\"\"\n",
    "    def __init__(self, quantization_channels: int = 256):\n",
    "        self.Q = quantization_channels\n",
    "        self.enc = torchaudio.transforms.MuLawEncoding(self.Q)\n",
    "        self.dec = torchaudio.transforms.MuLawDecoding(self.Q)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mu_law_encode(self, x: torch.Tensor) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Encode continuous audio waveform to discrete mu-law tokens.\n",
    "        \n",
    "        Converts continuous audio samples in [-1, 1] range to discrete integers\n",
    "        in [0, Q-1] range using mu-law companding.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input audio waveform, shape [..., T], \n",
    "                            dtype float, values in [-1, 1]\n",
    "        \n",
    "        Returns:\n",
    "            torch.LongTensor: Encoded tokens, shape [..., T], \n",
    "                            dtype long, values in [0, Q-1]\n",
    "        \n",
    "        Example:\n",
    "            >>> codec = MuLawEncoding(256)\n",
    "            >>> audio = torch.randn(1, 16000)  # 1 second at 16kHz\n",
    "            >>> tokens = codec.mu_law_encode(audio)  # shape: [1, 16000]\n",
    "        \"\"\"\n",
    "        return self.enc(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mu_law_decode(self, q: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode discrete mu-law tokens back to continuous audio waveform.\n",
    "        \n",
    "        Converts discrete tokens in [0, Q-1] range back to continuous audio\n",
    "        samples in [-1, 1] range using inverse mu-law expansion.\n",
    "        \n",
    "        Args:\n",
    "            q (torch.Tensor): Encoded tokens, shape [..., T], \n",
    "                            dtype long/int, values in [0, Q-1]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Decoded audio waveform, shape [..., T], \n",
    "                        dtype float, values in [-1, 1]\n",
    "        \n",
    "        Example:\n",
    "            >>> codec = MuLawEncoding(256)\n",
    "            >>> tokens = torch.randint(0, 256, (1, 16000))\n",
    "            >>> audio = codec.mu_law_decode(tokens)  # shape: [1, 16000]\n",
    "        \"\"\"\n",
    "        return self.dec(q)\n",
    "\n",
    "\n",
    "# Save codec once and use everywhere.\n",
    "codec = MuLawEncoding(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 2,  # CHANGED: Use 0 for debugging, multiprocessing can cause issues\n",
    "    \"pin_memory\": True,\n",
    "    \"mu\": 256,\n",
    "    \"sr\": 16000,\n",
    "    \"trim_silence_thresh\": 1e-3,\n",
    "    \"window_size\": 32001,  # ~2 seconds at 16kHz\n",
    "\n",
    "    # Wavenet architecture related.\n",
    "    \"residual_channels\": 64,\n",
    "    \"skip_channels\": 256,\n",
    "    \"output_dim\": 256,\n",
    "    \"n_layers\": 10,\n",
    "    \"n_blocks\": 5,\n",
    "    \"kernel_size\": 2,\n",
    "    'hop_size': 16000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_waveform(audio, title=\"\"):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(audio.t().numpy())\n",
    "    plt.title(f\"Audio: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def play_encoded_sample(tokens: torch.Tensor, sr: int = config['sr']) -> None:\n",
    "    # tokens: [T] or [1,T] long\n",
    "    if tokens.dim() == 1:\n",
    "        tokens = tokens.unsqueeze(0)\n",
    "    wav = codec.mu_law_decode(tokens).squeeze(0).cpu().numpy()  # in [-1, 1]\n",
    "    display(Audio(wav, rate=config['sr']))\n",
    "\n",
    "\n",
    "def play_batch(batch):\n",
    "    audio, _ = batch\n",
    "    audio = audio[0]  # Get first item from batch\n",
    "    play_encoded_sample(audio)\n",
    "\n",
    "\n",
    "def plot_encoded_sample(tokens: torch.Tensor, sr: int, title=\"\"):\n",
    "    wav = codec.mu_law_decode(tokens if tokens.dim(\n",
    "    ) == 2 else tokens.unsqueeze(0)).squeeze(0).cpu()\n",
    "    # your plotting util expects [1,T]\n",
    "    display_waveform(wav.unsqueeze(0), title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca148799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Audio preprocessing utilities for WaveNet training and inference.\n",
    "    \n",
    "    Handles audio resampling, normalization, silence trimming, segmentation,\n",
    "    and preparation of batched training data with mu-law encoding.\n",
    "    \n",
    "    Attributes:\n",
    "        mu_law_encoding (MuLawEncoding): Codec for mu-law encoding/decoding\n",
    "        resamplers (dict): Cache of resampler transforms for different sample rates\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mu_law_encoding = codec\n",
    "        self.resamplers = {}  # Cache resamplers for efficiency\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Normalize audio to [-1, 1] range using peak and RMS normalization.\n",
    "        \n",
    "        Applies peak normalization to ensure maximum amplitude is 1.0, followed by\n",
    "        RMS normalization to standardize the energy level across different audio files.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input audio waveform, shape [channels, time] or [time],\n",
    "                            any dtype (will be converted to float32)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Normalized audio, shape same as input, dtype float32,\n",
    "                        values in approximately [-1, 1] range\n",
    "        \n",
    "        Example:\n",
    "            >>> processor = AudioProcessor()\n",
    "            >>> audio = torch.randn(1, 16000) * 10  # Loud audio\n",
    "            >>> normalized = processor.normalize(audio)  # Peak around ±1.0\n",
    "        \"\"\"\n",
    "        # Ensure audio is float32 and normalize to [-1, 1]\n",
    "        if x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "\n",
    "        # Peak normalization: scale to maximum absolute value of 1.0\n",
    "        max_val = torch.max(torch.abs(x))\n",
    "        if max_val > 0:  # Avoid division by zero\n",
    "            x = x / max_val\n",
    "\n",
    "        # RMS normalization: standardize energy level\n",
    "        target_rms = 0.1\n",
    "\n",
    "        def rms(x):\n",
    "            return torch.sqrt(torch.mean(x**2) + 1e-8)\n",
    "        current_rms = rms(x)\n",
    "        if current_rms > 0:  # Avoid division by zero\n",
    "            x = x * (target_rms / current_rms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def resample_audio(self, audio, orig_sr, target_sr):\n",
    "        \"\"\"\n",
    "        Resample audio to target sample rate with caching for efficiency.\n",
    "        \n",
    "        Uses cached resampler transforms to avoid recreating them for the same\n",
    "        sample rate pairs, which significantly improves performance when processing\n",
    "        many audio files.\n",
    "        \n",
    "        Args:\n",
    "            audio (torch.Tensor): Input audio waveform, shape [channels, time]\n",
    "            orig_sr (int): Original sample rate in Hz\n",
    "            target_sr (int): Target sample rate in Hz\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Resampled audio, shape [channels, new_time] where\n",
    "                        new_time = time * (target_sr / orig_sr)\n",
    "        \n",
    "        Example:\n",
    "            >>> processor = AudioProcessor()\n",
    "            >>> audio_44k = torch.randn(1, 44100)  # 1 second at 44.1kHz\n",
    "            >>> audio_16k = processor.resample_audio(audio_44k, 44100, 16000)\n",
    "            >>> print(audio_16k.shape)  # torch.Size([1, 16000])\n",
    "        \"\"\"\n",
    "        if orig_sr == target_sr:\n",
    "            return audio\n",
    "\n",
    "        # Use cached resampler for efficiency\n",
    "        resampler_key = f\"{orig_sr}_{target_sr}\"\n",
    "        if resampler_key not in self.resamplers:\n",
    "            self.resamplers[resampler_key] = torchaudio.transforms.Resample(\n",
    "                orig_freq=orig_sr,\n",
    "                new_freq=target_sr\n",
    "            )\n",
    "\n",
    "        return self.resamplers[resampler_key](audio)\n",
    "\n",
    "    def trim_silence(self, sig, thresh=config['trim_silence_thresh']):\n",
    "        \"\"\"\n",
    "        Remove leading and trailing silence from audio signal.\n",
    "        \n",
    "        Detects silence by computing energy (absolute value) and trimming \n",
    "        samples below the threshold from the beginning and end of the signal.\n",
    "        \n",
    "        Args:\n",
    "            sig (torch.Tensor): Input audio signal, shape [1, time]\n",
    "            thresh (float): Energy threshold below which samples are considered silence.\n",
    "                          Default from config['trim_silence_thresh']\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Trimmed audio signal, shape [1, trimmed_time] where\n",
    "                        trimmed_time <= time\n",
    "        \n",
    "        Example:\n",
    "            >>> processor = AudioProcessor()\n",
    "            >>> # Audio with silence padding\n",
    "            >>> audio = torch.cat([torch.zeros(1, 1000), torch.randn(1, 8000), \n",
    "            ...                    torch.zeros(1, 1000)], dim=1)\n",
    "            >>> trimmed = processor.trim_silence(audio, thresh=0.01)\n",
    "            >>> print(f\"Original: {audio.shape[1]}, Trimmed: {trimmed.shape[1]}\")\n",
    "        \"\"\"\n",
    "        # Calculate energy (absolute value)\n",
    "        energy = sig.abs().squeeze()\n",
    "        # Find indices where energy is above threshold\n",
    "        idx = torch.where(energy > thresh)[0]\n",
    "        if len(idx) == 0:\n",
    "            return sig  # Return original if no samples above threshold\n",
    "        # Return trimmed signal\n",
    "        return sig[:, idx[0].item():idx[-1].item() + 1]\n",
    "\n",
    "    def segment_audio(self, audio, drop_last=True, hop_size=None):\n",
    "        \"\"\"\n",
    "        Split audio into overlapping or non-overlapping segments for training.\n",
    "        \n",
    "        Divides long audio sequences into fixed-size windows that can be processed\n",
    "        independently. Supports overlapping windows to increase training data.\n",
    "        \n",
    "        Args:\n",
    "            audio (torch.Tensor): Input audio, shape [1, time]\n",
    "            drop_last (bool): If True, drop incomplete segments at the end.\n",
    "                            If False, pad the last segment. Default True.\n",
    "            hop_size (int, optional): Step size between segments. If None,\n",
    "                                    uses window_size (non-overlapping).\n",
    "                                    If < window_size, creates overlapping segments.\n",
    "        \n",
    "        Returns:\n",
    "            list[torch.Tensor]: List of audio segments, each with shape [1, window_size]\n",
    "                              where window_size = config['window_size']\n",
    "        \n",
    "        Example:\n",
    "            >>> processor = AudioProcessor()\n",
    "            >>> audio = torch.randn(1, 100000)  # Long audio\n",
    "            >>> segments = processor.segment_audio(audio, hop_size=16000)\n",
    "            >>> print(f\"Created {len(segments)} segments\")\n",
    "            >>> print(f\"Each segment shape: {segments[0].shape}\")  # [1, 32001]\n",
    "        \"\"\"\n",
    "        window_size = config['window_size']\n",
    "        hop = hop_size or window_size\n",
    "        T = audio.shape[1]\n",
    "\n",
    "        segments = []\n",
    "        # Extract all full windows\n",
    "        for start in range(0, T - window_size + 1, hop):\n",
    "            segments.append(audio[:, start:start + window_size])\n",
    "\n",
    "        # Handle incomplete tail segment\n",
    "        if not drop_last and (T < window_size or (T - window_size) % hop != 0):\n",
    "            last_start = max(0, T - window_size)\n",
    "            tail = audio[:, last_start:]\n",
    "            if tail.shape[1] < window_size:\n",
    "                tail = F.pad(tail, (0, window_size - tail.shape[1]))\n",
    "            segments.append(tail)\n",
    "\n",
    "        return segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Split dataset into train and test sets (90-10 split)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_indices = list(range(0, train_size))\n",
    "test_indices = list(range(train_size, len(dataset)))\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "fake_train_dataset, fake_test_dataset = Subset(\n",
    "    dataset, [0]), Subset(dataset, [0])\n",
    "\n",
    "print(f\"Fake training set size: {len(fake_train_dataset)}\")\n",
    "print(f\"Fake test set size: {len(fake_test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a963ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processor.\n",
    "audio_processor = AudioProcessor()\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "# Fake dataloaders to test model training/overfitting on just 1 sample.\n",
    "fake_train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "fake_test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch[0].shape)\n",
    "    play_encoded_sample(batch[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7007405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity(x, y):\n",
    "    print(\"x dtype:\", x.dtype, \"min:\", int(x.min()), \"max:\", int(x.max()))\n",
    "    print(\"y dtype:\", y.dtype, \"min:\", int(y.min()), \"max:\", int(y.max()))\n",
    "    assert x.dtype == torch.long and y.dtype == torch.long\n",
    "    assert (x >= 0).all() and (x <= 255).all()\n",
    "    assert (y >= 0).all() and (y <= 255).all()\n",
    "\n",
    "\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    sanity(x, y)\n",
    "    if i > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalDilatedConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal dilated 1D convolution for autoregressive sequence modeling.\n",
    "    \n",
    "    Implements dilated convolution with causal padding to ensure that the output\n",
    "    at time t only depends on inputs at times <= t. This preserves the autoregressive\n",
    "    property needed for WaveNet's generative modeling.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        kernel_size (int): Size of the convolving kernel. Default: 2\n",
    "        dilation (int): Dilation factor for dilated convolution. Default: 1\n",
    "        \n",
    "    Attributes:\n",
    "        kernel_size (int): Stored kernel size\n",
    "        dilation (int): Stored dilation factor\n",
    "        conv1d (nn.Conv1d): The underlying convolution layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            padding=0,  # No automatic padding - we handle causality manually\n",
    "            bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply causal dilated convolution to input sequence.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor, shape [batch_size, in_channels, time]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor, shape [batch_size, out_channels, time]\n",
    "                        Same temporal length as input due to causal padding\n",
    "                        \n",
    "        Note:\n",
    "            Causal padding of size (kernel_size - 1) * dilation is applied to the left\n",
    "            (past) side only, ensuring future information doesn't leak into past predictions.\n",
    "        \"\"\"\n",
    "        if self.kernel_size > 1:\n",
    "            # Apply causal padding: pad left side only to prevent future leakage\n",
    "            pad_left = (self.kernel_size - 1) * self.dilation\n",
    "            x = F.pad(x, (pad_left, 0), mode='constant', value=0)\n",
    "        return self.conv1d(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    WaveNet residual block with gated activation and skip connections.\n",
    "    \n",
    "    Core building block of WaveNet architecture. Applies dilated causal convolution\n",
    "    followed by gated activation (tanh * sigmoid), then projects to residual and \n",
    "    skip connection outputs. The residual path enables deep networks while skip\n",
    "    connections aggregate features across all layers.\n",
    "    \n",
    "    Args:\n",
    "        C_res (int): Number of residual channels (internal feature dimension)\n",
    "        C_skip (int): Number of skip connection channels\n",
    "        dilation (int): Dilation factor for the causal convolution. Default: 1\n",
    "        \n",
    "    Attributes:\n",
    "        dilated_conv (CausalDilatedConvolution): Causal dilated convolution layer\n",
    "        skip_conv1x1 (nn.Conv1d): 1x1 conv for skip connection projection  \n",
    "        res_conv1x1 (nn.Conv1d): 1x1 conv for residual connection projection\n",
    "    \"\"\"\n",
    "    def __init__(self, C_res, C_skip, dilation=1):\n",
    "        super().__init__()\n",
    "        # Dilated conv outputs 2*C_res channels for gated activation\n",
    "        self.dilated_conv = CausalDilatedConvolution(C_res, 2 * C_res, kernel_size=2, dilation=dilation)\n",
    "        self.skip_conv1x1 = nn.Conv1d(C_res, C_skip, 1)\n",
    "        self.res_conv1x1 = nn.Conv1d(C_res, C_res, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through residual block.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor, shape [batch_size, C_res, time]\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (residual_out, skip_out) where:\n",
    "                - residual_out: torch.Tensor, shape [batch_size, C_res, time]\n",
    "                              Residual output for next layer (x + processed_x)\n",
    "                - skip_out: torch.Tensor, shape [batch_size, C_skip, time]\n",
    "                          Skip connection output for final aggregation\n",
    "                          \n",
    "        Note:\n",
    "            The gated activation is computed as: tanh(filter) * sigmoid(gate)\n",
    "            where filter and gate are the two halves of the dilated conv output.\n",
    "            This allows the network to learn what information to pass through.\n",
    "        \"\"\"\n",
    "        # Apply dilated causal convolution\n",
    "        output = self.dilated_conv(x)  # [B, 2*C_res, T]\n",
    "        \n",
    "        # Split into filter and gate components for gated activation\n",
    "        filter_out, gate_out = torch.chunk(output, 2, dim=1)  # Each: [B, C_res, T]\n",
    "\n",
    "        # Apply gated activation unit: tanh(filter) ⊙ sigmoid(gate)\n",
    "        gated = torch.tanh(filter_out) * torch.sigmoid(gate_out)  # [B, C_res, T]\n",
    "\n",
    "        # Project to residual and skip connection outputs via 1x1 convolutions\n",
    "        residual = self.res_conv1x1(gated)  # [B, C_res, T]\n",
    "        skip = self.skip_conv1x1(gated)     # [B, C_skip, T]\n",
    "\n",
    "        # Return residual connection (input + processed) and skip output\n",
    "        return residual + x, skip\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627364d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wavenet(nn.Module):\n",
    "    \"\"\"\n",
    "    WaveNet: A Generative Model for Raw Audio.\n",
    "    \n",
    "    Implements the WaveNet architecture for autoregressive audio generation.\n",
    "    The model uses stacked dilated causal convolutions with residual and skip\n",
    "    connections to efficiently model long-range dependencies in audio sequences.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input embedding layer: maps discrete tokens to continuous features\n",
    "    - Stacked residual blocks: each block contains multiple layers with increasing dilations\n",
    "    - Skip connections: aggregate features from all layers\n",
    "    - Output head: projects skip features to output logits\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary containing:\n",
    "            - residual_channels (int): Number of channels in residual paths\n",
    "            - skip_channels (int): Number of channels in skip connections  \n",
    "            - output_dim (int): Vocabulary size (typically 256 for mu-law)\n",
    "            - n_layers (int): Number of layers per residual block\n",
    "            - n_blocks (int): Number of residual blocks\n",
    "            - kernel_size (int): Convolution kernel size (typically 2)\n",
    "            \n",
    "    Attributes:\n",
    "        config (dict): Stored configuration\n",
    "        C_res (int): Residual channel dimension\n",
    "        C_output (int): Output vocabulary size\n",
    "        C_skip (int): Skip connection channel dimension\n",
    "        kernel_size (int): Convolution kernel size\n",
    "        _rf (int): Computed receptive field size\n",
    "        embedding (nn.Embedding): Token embedding layer\n",
    "        conv1d (nn.Conv1d): Initial feature processing layer\n",
    "        residual_blocks (nn.ModuleList): Stack of residual blocks\n",
    "        output_head (nn.Sequential): Final output projection layers\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.C_res = config['residual_channels']\n",
    "        self.C_output = config['output_dim']\n",
    "        self.C_skip = config['skip_channels']\n",
    "        self.kernel_size = config['kernel_size']\n",
    "        self._rf = self._compute_receptive_field()\n",
    "\n",
    "        # Token embedding: converts discrete tokens [0, 255] to continuous features\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.C_output, self.C_res, dtype=torch.float32)\n",
    "\n",
    "        # Initial 1x1 convolution for feature processing and stability\n",
    "        self.conv1d = nn.Conv1d(self.C_res, self.C_res, kernel_size=1)\n",
    "\n",
    "        # Stack of residual blocks with exponentially increasing dilations\n",
    "        # Each block contains n_layers with dilations: 1, 2, 4, 8, ..., 2^(n_layers-1)\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            self._create_residual_block() for i in range(config['n_blocks'])\n",
    "        ])\n",
    "        \n",
    "        # Output head: processes aggregated skip connections to produce logits\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.C_skip, self.C_skip, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.C_skip, self.C_output, kernel_size=1))\n",
    "\n",
    "    def _create_residual_block(self):\n",
    "        \"\"\"\n",
    "        Create a single residual block with exponentially increasing dilations.\n",
    "        \n",
    "        Each block contains n_layers ResidualBlock modules with dilation factors\n",
    "        of 1, 2, 4, 8, ..., 2^(n_layers-1). This exponential growth allows the\n",
    "        model to efficiently capture both short and long-range dependencies.\n",
    "        \n",
    "        Returns:\n",
    "            nn.ModuleList: List of ResidualBlock modules with increasing dilations\n",
    "            \n",
    "        Example:\n",
    "            For n_layers=5: dilations = [1, 2, 4, 8, 16]\n",
    "            For n_layers=10: dilations = [1, 2, 4, ..., 512]\n",
    "        \"\"\"\n",
    "        return nn.ModuleList([\n",
    "            ResidualBlock(self.C_res, self.C_skip, dilation=2**i)\n",
    "            for i in range(self.config['n_layers'])\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through WaveNet model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input token sequence, shape [batch_size, time],\n",
    "                            dtype long, values in [0, output_dim-1]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits, shape [batch_size, time, output_dim],\n",
    "                        dtype float. These can be used with cross-entropy loss\n",
    "                        or softmax for sampling.\n",
    "                        \n",
    "        Processing flow:\n",
    "            1. Embed tokens to continuous features: [B, T] -> [B, T, C_res]\n",
    "            2. Transpose for convolution: [B, T, C_res] -> [B, C_res, T]\n",
    "            3. Apply initial 1x1 convolution\n",
    "            4. Process through residual blocks, accumulating skip connections\n",
    "            5. Apply output head to aggregated skip connections\n",
    "            6. Transpose back to sequence format: [B, C_output, T] -> [B, T, C_output]\n",
    "        \"\"\"\n",
    "        # Step 1: Embed discrete tokens to continuous features\n",
    "        x_embd = self.embedding(x)  # [B, T] -> [B, T, C_res]\n",
    "        \n",
    "        # Step 2: Transpose for convolution operations (PyTorch conv1d expects [B, C, T])\n",
    "        x_embd = x_embd.permute(0, 2, 1)  # [B, T, C_res] -> [B, C_res, T]\n",
    "        skip_output = None\n",
    "\n",
    "        # Step 3: Apply initial feature processing\n",
    "        x_embd = self.conv1d(x_embd)  # [B, C_res, T]\n",
    "\n",
    "        # Step 4: Process through all residual blocks\n",
    "        for residual_block in self.residual_blocks:\n",
    "            for layer in residual_block:\n",
    "                x_embd, x_skip = layer(x_embd)\n",
    "                # Accumulate skip connections from all layers\n",
    "                skip_output = x_skip if skip_output is None else skip_output + x_skip\n",
    "\n",
    "        # Step 5: Generate output logits from aggregated skip connections\n",
    "        output = self.output_head(skip_output)  # [B, C_skip, T] -> [B, C_output, T]\n",
    "\n",
    "        # Step 6: Transpose back to sequence format for loss computation\n",
    "        return output.permute(0, 2, 1)  # [B, C_output, T] -> [B, T, C_output]\n",
    "\n",
    "    def _compute_receptive_field(self):\n",
    "        \"\"\"\n",
    "        Compute the theoretical receptive field of the WaveNet model.\n",
    "        \n",
    "        The receptive field determines how many past time steps the model\n",
    "        can observe when predicting the next token. For WaveNet with\n",
    "        exponentially increasing dilations, this grows exponentially with depth.\n",
    "        \n",
    "        Returns:\n",
    "            int: Receptive field size in time steps\n",
    "            \n",
    "        Formula:\n",
    "            For kernel_size=2: RF = 1 + n_blocks * (2^n_layers - 1)\n",
    "            General case: RF = 1 + n_blocks * sum_{i=0}^{n_layers-1} (k-1)*2^i\n",
    "            \n",
    "        Example:\n",
    "            n_blocks=5, n_layers=10, kernel_size=2:\n",
    "            RF = 1 + 5 * (2^10 - 1) = 1 + 5 * 1023 = 5116 time steps\n",
    "        \"\"\"\n",
    "        if self.kernel_size != 2:\n",
    "            # General formula for arbitrary kernel size\n",
    "            dilation_sum = (self.kernel_size - 1) * (2**self.config['n_layers'] - 1)\n",
    "            return 1 + self.config['n_blocks'] * dilation_sum\n",
    "        # Optimized formula for kernel_size=2\n",
    "        return 1 + self.config['n_blocks'] * (2**self.config['n_layers'] - 1)\n",
    "\n",
    "    def get_receptive_field(self):\n",
    "        \"\"\"\n",
    "        Get the receptive field size of the model.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of past time steps the model can observe\n",
    "        \"\"\"\n",
    "        return self._rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out dataloaders, etc. based on training requirements.\n",
    "# Fake dataset setup to test if training is working and is able to overfit on 1 sample.\n",
    "USE_FAKE = False\n",
    "if USE_FAKE:\n",
    "    config['batch_size'] = 1\n",
    "    config['num_workers'] = 0\n",
    "\n",
    "checkpoint_dir = './wavenet_checkpoints_fake' if USE_FAKE else './wavenet_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "train_loader = fake_train_loader if USE_FAKE else train_loader\n",
    "test_loader = fake_test_loader if USE_FAKE else test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Base class for WaveNet model wrappers with checkpoint management and compilation.\n",
    "    \n",
    "    Provides common functionality for model checkpoint handling, path management,\n",
    "    and PyTorch compilation for both training and evaluation modes.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Model configuration dictionary\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        checkpoint_dir (str): Directory for saving/loading checkpoints\n",
    "        \n",
    "    Attributes:\n",
    "        config (dict): Model configuration\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        model_train (torch.fx.GraphModule): Compiled model for training (None initially)\n",
    "        model_eval (torch.fx.GraphModule): Compiled model for evaluation (None initially)\n",
    "        checkpoint_dir (str): Directory for checkpoints\n",
    "    \"\"\"\n",
    "    def __init__(self, config, base_model, checkpoint_dir):\n",
    "        self.config = config\n",
    "        self.base_model = base_model\n",
    "        self.model_train = None\n",
    "        self.model_eval = None\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def _get_checkpoint_paths(self):\n",
    "        \"\"\"\n",
    "        Get paths to latest and best model checkpoints if they exist.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (latest_path, best_path) where each is either a valid\n",
    "                   file path string or None if the checkpoint doesn't exist\n",
    "        \"\"\"\n",
    "        latest_path = self._get_latest_checkpoint_path()\n",
    "        best_path = self._get_best_checkpoint_path()\n",
    "        latest_path = latest_path if os.path.exists(latest_path) else None\n",
    "        best_path = best_path if os.path.exists(best_path) else None\n",
    "        return latest_path, best_path\n",
    "\n",
    "    def _get_best_checkpoint_path(self):\n",
    "        \"\"\"\n",
    "        Get path to the best model checkpoint (lowest validation loss).\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to best model checkpoint file\n",
    "        \"\"\"\n",
    "        return os.path.join(self.checkpoint_dir, 'best_model.pth')\n",
    "\n",
    "    def _get_latest_checkpoint_path(self):\n",
    "        \"\"\"\n",
    "        Get path to the latest model checkpoint (most recent training state).\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to latest checkpoint file\n",
    "        \"\"\"\n",
    "        return os.path.join(self.checkpoint_dir, 'latest_checkpoint.pth')\n",
    "\n",
    "    def _compile_for_training(self):\n",
    "        \"\"\"\n",
    "        Compile the model for optimized training performance.\n",
    "        \n",
    "        Sets model to training mode and applies PyTorch compilation with\n",
    "        \"reduce-overhead\" mode for faster training.\n",
    "        \n",
    "        Returns:\n",
    "            torch.fx.GraphModule: Compiled model optimized for training\n",
    "        \"\"\"\n",
    "        self.base_model.train()\n",
    "        return torch.compile(self.base_model, mode=\"reduce-overhead\")\n",
    "\n",
    "    def _compile_for_eval(self):\n",
    "        \"\"\"\n",
    "        Compile the model for optimized evaluation/inference performance.\n",
    "        \n",
    "        Sets model to evaluation mode and applies PyTorch compilation with\n",
    "        \"reduce-overhead\" mode for faster inference.\n",
    "        \n",
    "        Returns:\n",
    "            torch.fx.GraphModule: Compiled model optimized for evaluation\n",
    "        \"\"\"\n",
    "        self.base_model.eval()\n",
    "        return torch.compile(self.base_model, mode=\"reduce-overhead\")\n",
    "\n",
    "    def get_receptive_field(self):\n",
    "        \"\"\"\n",
    "        Get the receptive field size of the underlying model.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of past time steps the model can observe\n",
    "        \"\"\"\n",
    "        return self.base_model.get_receptive_field()\n",
    "\n",
    "\n",
    "class TrainableModel(Model):\n",
    "    \"\"\"\n",
    "    Trainable WaveNet model wrapper with training state management and checkpointing.\n",
    "    \n",
    "    Extends the base Model class to handle training-specific functionality including\n",
    "    optimizer state, learning rate scheduling, training metrics tracking, and\n",
    "    checkpoint loading/saving for resumable training.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Model configuration dictionary\n",
    "        checkpoint_dir (str): Directory for saving/loading checkpoints\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler\n",
    "        load_from_checkpoint (bool): Whether to load from existing checkpoint. Default: False\n",
    "        \n",
    "    Attributes:\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        optimizer (torch.optim.Optimizer): Training optimizer\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler\n",
    "        training_stats (dict): Training metrics history containing:\n",
    "            - train_losses (list): Training loss per epoch\n",
    "            - val_losses (list): Validation loss per epoch  \n",
    "            - train_accuracies (list): Training accuracy per epoch\n",
    "            - val_accuracies (list): Validation accuracy per epoch\n",
    "            - best_val_loss (float): Best validation loss achieved\n",
    "        trained_till_epoch_index (int): Last completed epoch (-1 if untrained)\n",
    "        compiled_model (torch.fx.GraphModule): Compiled model for training\n",
    "    \"\"\"\n",
    "    def __init__(self, config, checkpoint_dir, base_model, optimizer, scheduler, load_from_checkpoint=False):\n",
    "        super().__init__(config, base_model, checkpoint_dir)\n",
    "\n",
    "        self.base_model = base_model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.training_stats = {\n",
    "            'train_losses': [],\n",
    "            'val_losses': [],\n",
    "            'train_accuracies': [],\n",
    "            'val_accuracies': [],\n",
    "            'best_val_loss': float('inf'),\n",
    "        }\n",
    "        # Track training progress: -1 = untrained, 0+ = last completed epoch index\n",
    "        self.trained_till_epoch_index = -1\n",
    "        if load_from_checkpoint:\n",
    "            self._load_from_checkpoint()\n",
    "\n",
    "        # Compile model specifically for training optimization\n",
    "        self.compiled_model = self._compile_for_training()\n",
    "\n",
    "    def save(self, epoch, training_stats, learning_rate, is_best=False):\n",
    "        \"\"\"\n",
    "        Save complete training checkpoint with model and optimizer state.\n",
    "        \n",
    "        Saves all necessary information to resume training including model weights,\n",
    "        optimizer state, scheduler state, training metrics, and configuration.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): Current epoch number (0-indexed)\n",
    "            training_stats (dict): Dictionary containing training metrics:\n",
    "                - train_losses, val_losses, train_accuracies, val_accuracies, best_val_loss\n",
    "            learning_rate (float): Current learning rate\n",
    "            is_best (bool): Whether this is the best model so far (lowest val loss)\n",
    "            \n",
    "        Saves:\n",
    "            - Latest checkpoint: Always saved for training resumption\n",
    "            - Best checkpoint: Only saved when is_best=True for evaluation\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'config': self.config,\n",
    "            'model_state_dict': self.base_model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "\n",
    "            'train_losses': training_stats['train_losses'],\n",
    "            'val_losses': training_stats['val_losses'],\n",
    "            'train_accuracies': training_stats['train_accuracies'],\n",
    "            'val_accuracies': training_stats['val_accuracies'],\n",
    "            'best_val_loss': training_stats['best_val_loss'],\n",
    "\n",
    "            'trained_till_epoch_index': epoch,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "\n",
    "        # Always save latest checkpoint for resumable training\n",
    "        torch.save(checkpoint, self._get_latest_checkpoint_path())\n",
    "\n",
    "        # Save best model checkpoint for evaluation/inference\n",
    "        if is_best:\n",
    "            torch.save(checkpoint, self._get_best_checkpoint_path())\n",
    "            print(f\"💾 New best model saved! Val Loss: {\n",
    "                  training_stats['best_val_loss']:.4f}\")\n",
    "\n",
    "    def _load_from_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Load training state from the latest checkpoint for resumable training.\n",
    "        \n",
    "        Restores model weights, optimizer state, scheduler state, training metrics,\n",
    "        and epoch counter from the most recent checkpoint. If no checkpoint exists,\n",
    "        training will start from scratch.\n",
    "        \n",
    "        Updates:\n",
    "            - base_model: Loads saved model weights\n",
    "            - optimizer: Restores optimizer state (momentum, learning rates, etc.)\n",
    "            - scheduler: Restores scheduler state  \n",
    "            - training_stats: Loads training history metrics\n",
    "            - trained_till_epoch_index: Sets last completed epoch\n",
    "        \"\"\"\n",
    "        latest_checkpoint_path, _ = self._get_checkpoint_paths()\n",
    "\n",
    "        if not latest_checkpoint_path:\n",
    "            print(\"🆕 No existing checkpoints found. Starting training from scratch.\")\n",
    "            return\n",
    "\n",
    "        print(f\"🔄 Loading training checkpoint from: {latest_checkpoint_path}\")\n",
    "        checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
    "\n",
    "        # Restore model weights\n",
    "        self.base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Restore optimizer state (important for momentum, learning rates, etc.)\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Restore scheduler state if available\n",
    "        if 'scheduler_state_dict' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            \n",
    "        # Restore training progress\n",
    "        self.trained_till_epoch_index = checkpoint.get('trained_till_epoch_index', -1)\n",
    "\n",
    "        # Restore training metrics history\n",
    "        self.training_stats = {\n",
    "            'train_losses': checkpoint.get('train_losses', []),\n",
    "            'val_losses': checkpoint.get('val_losses', []),\n",
    "            'train_accuracies': checkpoint.get('train_accuracies', []),\n",
    "            'val_accuracies': checkpoint.get('val_accuracies', []),\n",
    "            'best_val_loss': checkpoint.get('best_val_loss', float('inf')),\n",
    "        }\n",
    "        print(f\"✅ Training checkpoint loaded successfully. Trained for {\n",
    "              self.trained_till_epoch_index + 1} epochs!\")\n",
    "\n",
    "\n",
    "class EvalModel(Model):\n",
    "    \"\"\"\n",
    "    Evaluation-only WaveNet model wrapper for inference and generation.\n",
    "    \n",
    "    Loads the best saved model checkpoint (lowest validation loss) and compiles\n",
    "    it for optimized evaluation performance. Used for inference, generation,\n",
    "    and model evaluation after training.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Model configuration dictionary\n",
    "        checkpoint_dir (str): Directory containing saved checkpoints\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        \n",
    "    Attributes:\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        compiled_model (torch.fx.GraphModule): Compiled model optimized for evaluation\n",
    "    \"\"\"\n",
    "    def __init__(self, config, checkpoint_dir, base_model):\n",
    "        super().__init__(config, base_model, checkpoint_dir)\n",
    "        self.base_model = base_model\n",
    "        self._load_from_checkpoint()\n",
    "\n",
    "        # Compile model specifically for evaluation/inference optimization\n",
    "        self.compiled_model = self._compile_for_eval()\n",
    "\n",
    "    def _load_from_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Load the best model checkpoint for evaluation.\n",
    "        \n",
    "        Loads model weights from the best checkpoint (lowest validation loss)\n",
    "        rather than the latest checkpoint. This ensures optimal performance\n",
    "        for inference and generation tasks.\n",
    "        \n",
    "        Updates:\n",
    "            - base_model: Loads best model weights for evaluation\n",
    "        \"\"\"\n",
    "        _, best_checkpoint_path = self._get_checkpoint_paths()\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            print(\"🆕 No existing best checkpoint found for evaluation\")\n",
    "            return\n",
    "\n",
    "        print(f\"🔄 Loading eval model checkpoint from: {best_checkpoint_path}\")\n",
    "        checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Load only model weights (no optimizer state needed for evaluation)\n",
    "        self.base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        epoch = checkpoint.get('trained_till_epoch_index', 0)\n",
    "        print(f\"✅ Eval model checkpoint loaded successfully. Trained till epoch {\n",
    "              epoch + 1}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Comprehensive trainer for WaveNet autoregressive audio modeling.\n",
    "    \n",
    "    Handles the complete training pipeline including forward passes, loss computation,\n",
    "    validation, checkpointing, progress tracking, and visualization. Supports\n",
    "    mixed precision training, gradient clipping, and learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        base_model (Wavenet): The underlying WaveNet model\n",
    "        trainable_model (TrainableModel): Wrapper with training state management\n",
    "        config (dict): Training configuration dictionary\n",
    "        learning_rate (float): Initial learning rate\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler\n",
    "        checkpoint_dir (str): Directory for saving checkpoints and plots\n",
    "        train_loader (torch.utils.data.DataLoader): Training data loader\n",
    "        val_loader (torch.utils.data.DataLoader): Validation data loader\n",
    "        device (torch.device): Device for training (cuda/cpu)\n",
    "        \n",
    "    Attributes:\n",
    "        device (torch.device): Training device\n",
    "        base_model (Wavenet): Base WaveNet model\n",
    "        trainable_model (TrainableModel): Training wrapper\n",
    "        compiled_model (torch.fx.GraphModule): Compiled model for training\n",
    "        learning_rate (float): Learning rate\n",
    "        optimizer (torch.optim.Optimizer): Training optimizer\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): LR scheduler\n",
    "        checkpoint_dir (str): Checkpoint directory\n",
    "        train_loader (torch.utils.data.DataLoader): Training data\n",
    "        val_loader (torch.utils.data.DataLoader): Validation data\n",
    "        training_stats (dict): Training metrics history\n",
    "        start_epoch (int): Epoch to start/resume training from\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 trainable_model,\n",
    "                 config,\n",
    "                 learning_rate,\n",
    "                 optimizer,\n",
    "                 scheduler,\n",
    "                 checkpoint_dir,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 device):\n",
    "        self.device = device\n",
    "        self.base_model = base_model\n",
    "        self.trainable_model = trainable_model\n",
    "        self.compiled_model = self.trainable_model.compiled_model\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        # Reference to training metrics (shared with trainable_model)\n",
    "        self.training_stats = self.trainable_model.training_stats\n",
    "        # Determine starting epoch (resume from checkpoint or start fresh)\n",
    "        self.start_epoch = self.trainable_model.trained_till_epoch_index + 1\n",
    "\n",
    "    def prepare_batch(self, batch, device):\n",
    "        \"\"\"\n",
    "        Move batch tensors to the specified device for training.\n",
    "        \n",
    "        Args:\n",
    "            batch (tuple): Batch from DataLoader containing (audio_x, audio_y)\n",
    "                - audio_x: Input token sequences, shape [batch_size, seq_len]\n",
    "                - audio_y: Target token sequences, shape [batch_size, seq_len]  \n",
    "            device (torch.device): Target device (cuda/cpu)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (audio_x, audio_y) moved to device\n",
    "                - audio_x: torch.LongTensor, shape [batch_size, seq_len], device=device\n",
    "                - audio_y: torch.LongTensor, shape [batch_size, seq_len], device=device\n",
    "        \"\"\"\n",
    "        audio_x, audio_y = batch\n",
    "        return audio_x.to(device), audio_y.to(device)\n",
    "\n",
    "    def calculate_accuracy(self, output, target):\n",
    "        \"\"\"\n",
    "        Calculate token-level prediction accuracy for monitoring training progress.\n",
    "        \n",
    "        Computes the fraction of tokens where the model's prediction (argmax of logits)\n",
    "        matches the ground truth target token.\n",
    "        \n",
    "        Args:\n",
    "            output (torch.Tensor): Model output logits, shape [batch_size * seq_len, vocab_size]\n",
    "            target (torch.Tensor): Target tokens, shape [batch_size * seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            float: Accuracy as fraction of correct predictions in [0, 1]\n",
    "        \"\"\"\n",
    "        pred = torch.argmax(output, dim=-1)  # Get predicted token indices\n",
    "        correct = (pred == target).sum().item()  # Count correct predictions\n",
    "        total = target.numel()  # Total number of tokens\n",
    "        return correct / total\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one complete epoch with mixed precision and gradient clipping.\n",
    "        \n",
    "        Processes all training batches, computing forward passes, losses, and gradients.\n",
    "        Uses automatic mixed precision (AMP) with FP16 for memory efficiency and speed,\n",
    "        gradient clipping for stability, and progress tracking.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (average_loss, average_accuracy) for the epoch\n",
    "                - average_loss (float): Mean cross-entropy loss across all batches\n",
    "                - average_accuracy (float): Mean token accuracy across all batches\n",
    "                \n",
    "        Training pipeline per batch:\n",
    "            1. Forward pass with mixed precision (FP16)\n",
    "            2. Compute cross-entropy loss on flattened sequences\n",
    "            3. Backward pass with gradient scaling\n",
    "            4. Gradient clipping (norm=1.0) for stability\n",
    "            5. Optimizer step with gradient unscaling\n",
    "        \"\"\"\n",
    "        self.base_model.train()\n",
    "        self.compiled_model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in pbar:\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                audio_x, audio_y = self.prepare_batch(batch, self.device)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Forward pass with automatic mixed precision (FP16)\n",
    "                with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                    output = self.compiled_model(audio_x)  # [B, T, vocab_size]\n",
    "                    B, T, C = output.shape\n",
    "                    \n",
    "                    # Flatten for cross-entropy loss computation\n",
    "                    output_flat = output.reshape(-1, C)      # [B*T, vocab_size]\n",
    "                    target_flat = audio_y.reshape(-1)        # [B*T]\n",
    "                    \n",
    "                    # Compute cross-entropy loss (supports FP16)\n",
    "                    loss = F.cross_entropy(output_flat, target_flat)\n",
    "\n",
    "                # Calculate accuracy for monitoring\n",
    "                accuracy = self.calculate_accuracy(output_flat, target_flat)\n",
    "\n",
    "                # Backward pass with gradient scaling for mixed precision\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(self.optimizer)\n",
    "                \n",
    "                # Gradient clipping for training stability\n",
    "                torch.nn.utils.clip_grad_norm_(self.base_model.parameters(), 1.0)\n",
    "                \n",
    "                # Optimizer step with automatic scaling\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # Accumulate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy\n",
    "                num_batches += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{accuracy:.3f}'\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in training batch: {e}\")\n",
    "                continue\n",
    "\n",
    "        return total_loss / max(num_batches, 1), total_accuracy / max(num_batches, 1)\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        \"\"\"\n",
    "        Validate the model for one complete epoch without gradient computation.\n",
    "        \n",
    "        Evaluates model performance on validation data using the same loss and\n",
    "        accuracy metrics as training. Uses mixed precision for consistency and\n",
    "        efficiency, but disables gradient computation for speed and memory savings.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (average_loss, average_accuracy) for validation epoch\n",
    "                - average_loss (float): Mean cross-entropy loss across validation batches\n",
    "                - average_accuracy (float): Mean token accuracy across validation batches\n",
    "                \n",
    "        Validation pipeline per batch:\n",
    "            1. Forward pass with mixed precision (FP16) and no gradients\n",
    "            2. Compute cross-entropy loss on flattened sequences\n",
    "            3. Calculate accuracy metrics for monitoring\n",
    "        \"\"\"\n",
    "        self.base_model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            pbar = tqdm(self.val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "            for batch in pbar:\n",
    "                try:\n",
    "                    # Move batch to device\n",
    "                    audio_x, audio_y = self.prepare_batch(batch, self.device)\n",
    "                    \n",
    "                    # Forward pass with mixed precision (no gradients)\n",
    "                    with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "                        output = self.base_model(audio_x)  # [B, T, vocab_size]\n",
    "                        B, T, C = output.shape\n",
    "                        \n",
    "                        # Flatten for loss computation\n",
    "                        output_flat = output.reshape(-1, C)      # [B*T, vocab_size]\n",
    "                        target_flat = audio_y.reshape(-1)        # [B*T]\n",
    "                        \n",
    "                        # Compute validation loss\n",
    "                        loss = F.cross_entropy(output_flat, target_flat)\n",
    "\n",
    "                    # Calculate accuracy for monitoring\n",
    "                    accuracy = self.calculate_accuracy(output_flat, target_flat)\n",
    "\n",
    "                    # Accumulate metrics\n",
    "                    total_loss += loss.item()\n",
    "                    total_accuracy += accuracy\n",
    "                    num_batches += 1\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'Val Loss': f'{loss.item():.4f}',\n",
    "                        'Val Acc': f'{accuracy:.3f}'\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return total_loss / max(num_batches, 1), total_accuracy / max(num_batches, 1)\n",
    "\n",
    "    def plot_training_progress(self, epoch):\n",
    "        \"\"\"\n",
    "        Create and save comprehensive training progress visualization.\n",
    "        \n",
    "        Generates a multi-panel plot showing training/validation loss curves,\n",
    "        accuracy curves, and overfitting indicator. Saves plots to checkpoint\n",
    "        directory for monitoring training health and performance.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): Current epoch number (0-indexed) for plot title and filename\n",
    "            \n",
    "        Plots created:\n",
    "            1. Loss curves: Training and validation loss over epochs\n",
    "            2. Accuracy curves: Training and validation accuracy over epochs  \n",
    "            3. Overfitting indicator: Difference between validation and training loss\n",
    "            \n",
    "        Saves:\n",
    "            - PNG file: progress_epoch_{epoch+1}.png in checkpoint directory\n",
    "            - Displays plot in notebook if running interactively\n",
    "        \"\"\"\n",
    "        train_losses = self.training_stats['train_losses']\n",
    "        val_losses = self.training_stats['val_losses']\n",
    "        train_accuracies = self.training_stats['train_accuracies']\n",
    "        val_accuracies = self.training_stats['val_accuracies']\n",
    "\n",
    "        if len(train_losses) == 0:\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 3, 1)\n",
    "        epochs_range = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs_range, train_losses, 'b-',\n",
    "                 label='Training Loss', linewidth=2)\n",
    "        plt.plot(epochs_range, val_losses, 'r-',\n",
    "                 label='Validation Loss', linewidth=2)\n",
    "        plt.title(f'Training Progress (Epoch {epoch+1})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(epochs_range, train_accuracies, 'b-',\n",
    "                 label='Training Accuracy', linewidth=2)\n",
    "        plt.plot(epochs_range, val_accuracies, 'r-',\n",
    "                 label='Validation Accuracy', linewidth=2)\n",
    "        plt.title(f'Accuracy Progress (Epoch {epoch+1})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Loss difference (overfitting indicator)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        loss_diff = [v - t for t, v in zip(train_losses, val_losses)]\n",
    "        plt.plot(epochs_range, loss_diff, 'g-', linewidth=2)\n",
    "        plt.title('Overfitting Indicator (Val - Train Loss)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss Difference')\n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(checkpoint_dir,\n",
    "                    f'progress_epoch_{epoch+1}.png'), dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Execute the complete training loop with checkpointing and progress tracking.\n",
    "        \n",
    "        Runs training for the specified number of epochs, handling resumption from\n",
    "        checkpoints, validation, learning rate scheduling, progress visualization,\n",
    "        and automatic saving of best models.\n",
    "        \n",
    "        Args:\n",
    "            num_epochs (int): Total number of epochs to train for. If model has\n",
    "                            already been trained beyond this, training is skipped.\n",
    "                            \n",
    "        Training flow per epoch:\n",
    "            1. Training phase: Process all training batches\n",
    "            2. Validation phase: Evaluate on validation data\n",
    "            3. Learning rate scheduling: Update LR based on validation loss\n",
    "            4. Checkpointing: Save latest model and best model if improved\n",
    "            5. Progress visualization: Plot metrics every 5 epochs\n",
    "            6. Training summary: Save final results to JSON\n",
    "            \n",
    "        Outputs:\n",
    "            - Checkpoint files: latest_checkpoint.pth, best_model.pth\n",
    "            - Progress plots: progress_epoch_*.png files\n",
    "            - Training summary: training_summary.json with final metrics\n",
    "        \"\"\"\n",
    "        # Check if training is already complete\n",
    "        if self.start_epoch >= num_epochs:\n",
    "            print(f\"Model is already trained to {num_epochs} epochs.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🚀 Training...\")\n",
    "        print(f\"📊 Epochs: {self.start_epoch} → {num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "\n",
    "            # Update scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Save metrics\n",
    "            self.training_stats['train_losses'].append(train_loss)\n",
    "            self.training_stats['val_losses'].append(val_loss)\n",
    "            self.training_stats['train_accuracies'].append(train_acc)\n",
    "            self.training_stats['val_accuracies'].append(val_acc)\n",
    "\n",
    "            # Check for best model\n",
    "            is_best = val_loss < self.training_stats['best_val_loss']\n",
    "            if is_best:\n",
    "                self.training_stats['best_val_loss'] = val_loss\n",
    "\n",
    "            # Save checkpoint every epoch.\n",
    "            self.trainable_model.save(\n",
    "                epoch, self.training_stats, self.learning_rate, is_best)\n",
    "\n",
    "            # Print epoch summary\n",
    "            improvement = \"🔥\" if is_best else \"\"\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {\n",
    "                  val_acc:.3f} {improvement}\")\n",
    "            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "            # Plot every 5 epochs or if it's the first resumed epoch\n",
    "            if (epoch + 1) % 5 == 0 or epoch == self.start_epoch:\n",
    "                self.plot_training_progress(epoch)\n",
    "\n",
    "        print(f\"\\n🎉 Training completed!\")\n",
    "        print(f\"Best validation loss: {\n",
    "              self.training_stats['best_val_loss']:.4f}\")\n",
    "        print(f\"Total epochs trained: {\n",
    "              len(self.training_stats['train_losses'])}\")\n",
    "        print(f\"Checkpoints saved in: {self.checkpoint_dir}\")\n",
    "\n",
    "        # Final comprehensive plot.\n",
    "        self.plot_training_progress(\n",
    "            len(self.training_stats['train_losses']) - 1)\n",
    "\n",
    "        self._save_training_summary()\n",
    "        print(f\"Training completed!\")\n",
    "\n",
    "    def _save_training_summary(self):\n",
    "        \"\"\"\n",
    "        Save comprehensive training summary to JSON file.\n",
    "        \n",
    "        Creates a summary of the complete training session including final metrics,\n",
    "        best performance achieved, and total training duration. Useful for comparing\n",
    "        different training runs and tracking model performance.\n",
    "        \n",
    "        Saves:\n",
    "            training_summary.json containing:\n",
    "                - total_epochs: Number of epochs trained\n",
    "                - best_val_loss: Best validation loss achieved  \n",
    "                - final_train_loss: Training loss at end of training\n",
    "                - final_val_loss: Validation loss at end of training\n",
    "                - final_train_acc: Training accuracy at end of training\n",
    "                - final_val_acc: Validation accuracy at end of training\n",
    "        \"\"\"\n",
    "        final_results = {\n",
    "            'total_epochs': len(self.training_stats['train_losses']),\n",
    "            'best_val_loss': self.training_stats['best_val_loss'],\n",
    "            'final_train_loss': self.training_stats['train_losses'][-1] if self.training_stats['train_losses'] else None,\n",
    "            'final_val_loss': self.training_stats['val_losses'][-1] if self.training_stats['val_losses'] else None,\n",
    "            'final_train_acc': self.training_stats['train_accuracies'][-1] if self.training_stats['train_accuracies'] else None,\n",
    "            'final_val_acc': self.training_stats['val_accuracies'][-1] if self.training_stats['val_accuracies'] else None\n",
    "        }\n",
    "        with open(os.path.join(self.checkpoint_dir, 'training_summary.json'), 'w') as f:\n",
    "            json.dump(final_results, f, indent=2)\n",
    "        print(f\"📋 Training summary saved to: {\n",
    "              os.path.join(self.checkpoint_dir, 'training_summary.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train for.\n",
    "# If the saved checkpoint is < num_epochs then training will be done.\n",
    "# Otherwise model will be loaded and put in eval mode and compiled.\n",
    "# 180 is enough to overfit completely.\n",
    "num_epochs = 50 if not USE_FAKE else 180\n",
    "\n",
    "# Initialize base model and optimizer.\n",
    "base_model = Wavenet(config).to(device)\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=1, threshold=1e-3)\n",
    "\n",
    "# Load the latest trainable model.\n",
    "trainable_model = TrainableModel(config, checkpoint_dir, base_model, optimizer, scheduler,\n",
    "                                 load_from_checkpoint=True)\n",
    "print(f\"Model parameters: {sum(p.numel()\n",
    "      for p in trainable_model.base_model.parameters()):,}\")\n",
    "\n",
    "if trainable_model.trained_till_epoch_index + 1 < num_epochs:\n",
    "    print(f\"🚀 {'Resuming' if trainable_model.trained_till_epoch_index > -\n",
    "          1 else 'Starting'} WaveNet Training\")\n",
    "    # Create the trainer.\n",
    "    trainer = Trainer(base_model, trainable_model, config, learning_rate,\n",
    "                      optimizer, scheduler, checkpoint_dir, train_loader, test_loader, device)\n",
    "    trainer.train(num_epochs)\n",
    "else:\n",
    "    print(f\"Model is already trained to {num_epochs} epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model for evals and generation.\n",
    "base_model = Wavenet(config).to(device)\n",
    "model = EvalModel(config, checkpoint_dir, base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import prod\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_continuation(\n",
    "    model,\n",
    "    seed_tokens: torch.LongTensor,   # [B, T_seed], class IDs in [0..255]\n",
    "    n_steps: int,                    # how many new samples to generate\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    device: str | torch.device = \"cuda\",\n",
    "    rf_override: int | None = None,  # optionally pass model RF\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate audio continuation using autoregressive sampling from WaveNet.\n",
    "    \n",
    "    Performs autoregressive generation by iteratively predicting the next token\n",
    "    based on the current sequence context. Uses temperature and top-k sampling\n",
    "    for controllable generation quality vs diversity trade-off.\n",
    "    \n",
    "    Args:\n",
    "        model: WaveNet model with get_receptive_field() method\n",
    "        seed_tokens (torch.LongTensor): Initial sequence, shape [batch_size, seed_length],\n",
    "                                      dtype long, values in [0, 255]\n",
    "        n_steps (int): Number of new tokens to generate\n",
    "        temperature (float): Sampling temperature controlling randomness.\n",
    "                           1.0 = normal, < 1.0 = more deterministic, > 1.0 = more random\n",
    "        top_k (int, optional): If specified, only sample from top-k most likely tokens\n",
    "        device (str | torch.device): Device for computation\n",
    "        rf_override (int, optional): Override model's receptive field size\n",
    "        \n",
    "    Returns:\n",
    "        torch.LongTensor: Extended sequence, shape [batch_size, seed_length + n_steps],\n",
    "                        dtype long, values in [0, 255]\n",
    "        \n",
    "    Note:\n",
    "        - Uses mixed precision (FP16) for forward passes but FP32 for sampling stability\n",
    "        - Crops context to receptive field size for efficiency\n",
    "        - Supports only batch_size=1 for autoregressive generation\n",
    "        \n",
    "    Example:\n",
    "        >>> seed = torch.randint(0, 256, (1, 1000))  # 1000 token seed\n",
    "        >>> generated = generate_continuation(model, seed, n_steps=5000, temperature=0.8)\n",
    "        >>> print(generated.shape)  # torch.Size([1, 6000])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq = seed_tokens.to(device)  # [B, T_seed]\n",
    "    B = seq.size(0)\n",
    "    assert B == 1, \"start with batch size 1 for autoregressive generation\"\n",
    "\n",
    "    # try to infer RF from model config if not provided\n",
    "    if rf_override is not None:\n",
    "        rf = rf_override\n",
    "    else:\n",
    "        rf = model.get_receptive_field()\n",
    "\n",
    "    for _ in tqdm(range(n_steps), desc='Generating'):\n",
    "        # crop to receptive field context to save compute\n",
    "        ctx = seq[:, -rf:] if seq.size(1) > rf else seq\n",
    "\n",
    "        # Generate in fp32 for stability, torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "        with torch.inference_mode():\n",
    "            logits = model(ctx).float()          # [1, T_ctx, 256]\n",
    "            logits = logits[:, -1, :]    # [1, 256] last step\n",
    "\n",
    "        # temperature / top-k sampling (avoid argmax for better naturalness)\n",
    "        logits = logits / max(temperature, 1e-6)\n",
    "        if top_k is not None and top_k > 0:\n",
    "            topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)\n",
    "            probs = torch.zeros_like(\n",
    "                logits).scatter(-1, topk_idx, F.softmax(topk_vals, dim=-1))\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)  # [1,1] Long\n",
    "        seq = torch.cat([seq, next_tok], dim=1)             # append token\n",
    "\n",
    "    return seq  # [1, T_seed + n_steps] (tokens)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_greedy(model, seed, n_steps, device=\"cuda\", rf=None):\n",
    "    \"\"\"\n",
    "    Generate audio continuation using deterministic greedy decoding.\n",
    "    \n",
    "    Performs autoregressive generation by always selecting the most likely\n",
    "    next token (argmax). This produces deterministic, high-quality output\n",
    "    but may lack diversity compared to sampling methods.\n",
    "    \n",
    "    Args:\n",
    "        model: WaveNet model with get_receptive_field() method\n",
    "        seed (torch.LongTensor): Initial sequence, shape [batch_size, seed_length],\n",
    "                               dtype long, values in [0, 255]\n",
    "        n_steps (int): Number of new tokens to generate\n",
    "        device (str): Device for computation, default \"cuda\"\n",
    "        rf (int, optional): Receptive field size, will query model if None\n",
    "        \n",
    "    Returns:\n",
    "        torch.LongTensor: Extended sequence, shape [batch_size, seed_length + n_steps],\n",
    "                        dtype long, values in [0, 255]\n",
    "        \n",
    "    Note:\n",
    "        - Uses FP32 for argmax stability\n",
    "        - Crops context to receptive field for efficiency\n",
    "        - Deterministic output (no randomness)\n",
    "        \n",
    "    Example:\n",
    "        >>> seed = torch.randint(0, 256, (1, 1000))\n",
    "        >>> generated = generate_greedy(model, seed, n_steps=2000)\n",
    "        >>> print(generated.shape)  # torch.Size([1, 3000])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq = seed.to(device)\n",
    "    rf = rf or model.get_receptive_field()\n",
    "    for _ in tqdm(range(n_steps), desc='Generating'):\n",
    "        ctx = seq[:, -rf:] if seq.size(1) > rf else seq\n",
    "        # Force FP32 for numerical stability in argmax\n",
    "        logits = model(ctx)[:, -1, :].float()\n",
    "        nxt = logits.argmax(dim=-1, keepdim=True)  # Greedy selection\n",
    "        seq = torch.cat([seq, nxt], 1)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed11d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_continuation_fixed(\n",
    "    model,\n",
    "    seed_tokens: torch.LongTensor,   # [B, T_seed]\n",
    "    n_steps: int,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    device: str | torch.device = \"cuda\",\n",
    "    use_fp32: bool = True,  # Use FP32 for generation stability\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced generation function with improved numerical stability and debugging.\n",
    "    \n",
    "    An improved version of generate_continuation with better FP32 handling,\n",
    "    detailed progress logging, and numerical stability improvements for\n",
    "    high-quality audio generation.\n",
    "    \n",
    "    Args:\n",
    "        model: WaveNet model with get_receptive_field() method\n",
    "        seed_tokens (torch.LongTensor): Initial sequence, shape [batch_size, seed_length],\n",
    "                                      dtype long, values in [0, 255]\n",
    "        n_steps (int): Number of new tokens to generate\n",
    "        temperature (float): Sampling temperature. Default: 1.0\n",
    "        top_k (int, optional): Top-k filtering for sampling. Default: None (no filtering)\n",
    "        device (str | torch.device): Computation device. Default: \"cuda\"\n",
    "        use_fp32 (bool): Whether to use FP32 for generation stability. Default: True\n",
    "        \n",
    "    Returns:\n",
    "        torch.LongTensor: Extended sequence, shape [batch_size, seed_length + n_steps],\n",
    "                        dtype long, values in [0, 255]\n",
    "        \n",
    "    Features:\n",
    "        - Improved numerical stability with proper FP32 handling\n",
    "        - Progress logging every 1000 steps with entropy and probability metrics\n",
    "        - Robust top-k filtering implementation\n",
    "        - Better error handling and debugging output\n",
    "        \n",
    "    Example:\n",
    "        >>> seed = torch.randint(0, 256, (1, 2000))\n",
    "        >>> generated = generate_continuation_fixed(\n",
    "        ...     model, seed, n_steps=8000, temperature=0.9, top_k=100\n",
    "        ... )\n",
    "        >>> print(generated.shape)  # torch.Size([1, 10000])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq = seed_tokens.to(device)\n",
    "    B = seq.size(0)\n",
    "    assert B == 1, \"Use batch size 1 for generation\"\n",
    "\n",
    "    rf = model.get_receptive_field()\n",
    "    print(f\"Using receptive field: {rf}\")\n",
    "\n",
    "    for step in tqdm(range(n_steps), desc='Generating'):\n",
    "        # Crop to receptive field\n",
    "        ctx = seq[:, -rf:] if seq.size(1) > rf else seq\n",
    "\n",
    "        if use_fp32:\n",
    "            # Use FP32 for stability\n",
    "            with torch.inference_mode():\n",
    "                logits = model(ctx).float()  # Force FP32\n",
    "        else:\n",
    "            # Use mixed precision\n",
    "            with torch.inference_mode(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "                logits = model(ctx)\n",
    "                logits = logits.float()  # Convert to FP32 for sampling\n",
    "\n",
    "        logits = logits[:, -1, :]  # [1, 256] - last timestep\n",
    "\n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "        # Apply top-k filtering if specified\n",
    "        if top_k is not None and top_k > 0:\n",
    "            topk_vals, topk_idx = torch.topk(\n",
    "                logits, k=min(top_k, logits.size(-1)), dim=-1)\n",
    "            # Zero out non-top-k logits\n",
    "            logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "            logits_filtered.scatter_(-1, topk_idx, topk_vals)\n",
    "            logits = logits_filtered\n",
    "\n",
    "        # Sample next token\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "\n",
    "        # Append to sequence\n",
    "        seq = torch.cat([seq, next_tok], dim=1)\n",
    "\n",
    "        # Debug: Print some info every 1000 steps\n",
    "        if step % 1000 == 0:\n",
    "            prob_max = probs.max().item()\n",
    "            entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "            print(f\"Step {step}: max_prob={prob_max:.3f}, entropy={\n",
    "                  entropy:.3f}, token={next_tok.item()}\")\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_teacher_forcing_test(model, seed_tokens, target_tokens, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Test generation quality by comparing autoregressive vs teacher-forced predictions.\n",
    "    \n",
    "    Validates model consistency by comparing autoregressive generation with\n",
    "    teacher-forced predictions on the same sequence. High agreement indicates\n",
    "    good model training and generation stability.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained WaveNet model\n",
    "        seed_tokens (torch.LongTensor): Initial sequence, shape [1, seed_length]\n",
    "        target_tokens (torch.LongTensor): Full target sequence for comparison,\n",
    "                                        shape [1, target_length]\n",
    "        device (str): Computation device. Default: \"cuda\"\n",
    "        \n",
    "    Returns:\n",
    "        torch.LongTensor: Generated sequence from autoregressive sampling,\n",
    "                        shape [1, generated_length]\n",
    "        \n",
    "    Prints:\n",
    "        - Match percentage between teacher forcing and autoregressive generation\n",
    "        - Quality assessment based on agreement level\n",
    "        \n",
    "    Example:\n",
    "        >>> seed = target[:, :1000]  # First 1000 tokens as seed\n",
    "        >>> generated = generate_with_teacher_forcing_test(model, seed, target)\n",
    "        Teacher forcing vs autoregressive match: 0.847\n",
    "        ✅ Good match between teacher forcing and autoregressive generation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Teacher forcing (what the model should predict)\n",
    "    with torch.inference_mode():\n",
    "        teacher_logits = model(target_tokens.to(device))  # [1, T, 256]\n",
    "\n",
    "    # Autoregressive generation\n",
    "    generated = seed_tokens.clone()\n",
    "    rf = model.get_receptive_field()\n",
    "\n",
    "    for i in tqdm(range(target_tokens.size(1) - seed_tokens.size(1))):\n",
    "        ctx = generated[:, -rf:] if generated.size(1) > rf else generated\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            logits = model(ctx.to(device)).float()\n",
    "            logits = logits[:, -1, :]  # Last timestep\n",
    "\n",
    "        # Use argmax for deterministic comparison\n",
    "        next_tok = logits.argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_tok.cpu()], dim=1)\n",
    "\n",
    "    # Compare predictions\n",
    "    teacher_preds = teacher_logits.argmax(dim=-1)  # [1, T]\n",
    "\n",
    "    # Compare the overlapping region\n",
    "    overlap_start = seed_tokens.size(1)\n",
    "    overlap_end = min(generated.size(1), teacher_preds.size(1))\n",
    "\n",
    "    if overlap_end > overlap_start:\n",
    "        generated_overlap = generated[:, overlap_start:overlap_end]\n",
    "        teacher_overlap = teacher_preds[:, overlap_start:overlap_end]\n",
    "\n",
    "        matches = (generated_overlap == teacher_overlap.cpu()).float().mean()\n",
    "        print(f\"Teacher forcing vs autoregressive match: {matches:.3f}\")\n",
    "\n",
    "        if matches < 0.5:\n",
    "            print(\"⚠️  Poor match between teacher forcing and autoregressive generation\")\n",
    "        else:\n",
    "            print(\"✅ Good match between teacher forcing and autoregressive generation\")\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "\n",
    "def test_generation_quality(model, dataset, audio_processor, device):\n",
    "    \"\"\"\n",
    "    Comprehensive test suite for evaluating generation quality and model behavior.\n",
    "    \n",
    "    Performs multiple tests to assess generation quality including teacher forcing\n",
    "    consistency, temperature sensitivity, and mode collapse detection. Provides\n",
    "    detailed diagnostics for model debugging and validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained WaveNet model for testing\n",
    "        dataset: Dataset to sample test audio from\n",
    "        audio_processor (AudioProcessor): Audio preprocessing utilities\n",
    "        device (torch.device): Computation device\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Sample generation from the test suite\n",
    "        \n",
    "    Tests performed:\n",
    "        1. Teacher forcing consistency: Compare autoregressive vs teacher-forced predictions\n",
    "        2. Temperature sensitivity: Test generation diversity across different temperatures\n",
    "        3. Mode collapse detection: Check for identical outputs across multiple runs\n",
    "        \n",
    "    Example:\n",
    "        >>> test_result = test_generation_quality(model, dataset, audio_processor, device)\n",
    "        === Testing Generation Quality ===\n",
    "        1. Testing teacher forcing consistency...\n",
    "        2. Testing different temperatures...\n",
    "        3. Testing for mode collapse...\n",
    "        ✅ Generations show diversity\n",
    "    \"\"\"\n",
    "    print(\"=== Testing Generation Quality ===\")\n",
    "\n",
    "    # Get a test sample\n",
    "    audio, sr, _, _ = dataset[0]\n",
    "    audio = audio_processor.resample_audio(audio, sr, 16000)\n",
    "    audio = audio_processor.normalize(audio)\n",
    "\n",
    "    # Encode to tokens\n",
    "    mu_law = codec\n",
    "    tokens = mu_law.mu_law_encode(audio.squeeze()).unsqueeze(0)  # [1, T]\n",
    "\n",
    "    # Split into seed and target\n",
    "    split_point = min(8000, tokens.size(1) // 2)  # Use first half as seed\n",
    "    seed = tokens[:, :split_point]\n",
    "    target = tokens[:, :split_point + 1000]  # Small target for testing\n",
    "\n",
    "    print(f\"Seed length: {seed.size(1)}\")\n",
    "    print(f\"Target length: {target.size(1)}\")\n",
    "\n",
    "    # Test 1: Teacher forcing vs autoregressive\n",
    "    print(\"\\n1. Testing teacher forcing consistency...\")\n",
    "    generated = generate_with_teacher_forcing_test(model, seed, target, device)\n",
    "\n",
    "    # Test 2: Different temperatures\n",
    "    print(\"\\n2. Testing different temperatures...\")\n",
    "    for temp in [0.1, 0.8, 1.0, 1.2]:\n",
    "        print(f\"Temperature {temp}:\")\n",
    "        gen_seq = generate_continuation_fixed(\n",
    "            model, seed, n_steps=100, temperature=temp, device=device\n",
    "        )\n",
    "\n",
    "        # Check diversity\n",
    "        unique_tokens = gen_seq[:, seed.size(1):].unique().numel()\n",
    "        print(f\"  Unique tokens in 100 steps: {unique_tokens}\")\n",
    "\n",
    "        if temp > 1.0 and unique_tokens < 10:\n",
    "            print(f\"  ⚠️  Low diversity at temperature {temp}\")\n",
    "\n",
    "    # Test 3: Check for mode collapse\n",
    "    print(\"\\n3. Testing for mode collapse...\")\n",
    "    generations = []\n",
    "    for i in range(5):\n",
    "        gen_seq = generate_continuation_fixed(\n",
    "            model, seed, n_steps=200, temperature=1.0, device=device\n",
    "        )\n",
    "        generations.append(gen_seq[:, seed.size(1):])\n",
    "\n",
    "    # Check if all generations are identical\n",
    "    all_same = all(torch.equal(generations[0], gen) for gen in generations[1:])\n",
    "    if all_same:\n",
    "        print(\"⚠️  Mode collapse detected - all generations identical\")\n",
    "    else:\n",
    "        print(\"✅ Generations show diversity\")\n",
    "\n",
    "    return generations[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(i):\n",
    "    # Get one batch from test_loader;\n",
    "    audio, _ = next(iter(test_loader))     # x: [B, T-1] tokens (Long)\n",
    "    audio = audio[i, :].unsqueeze(0).to(device)\n",
    "\n",
    "    print(audio.shape)\n",
    "\n",
    "    play_encoded_sample(audio.cpu())\n",
    "    display_waveform(audio.cpu(), \"Original\")\n",
    "\n",
    "    # Seconds to seed the generation with.\n",
    "    seconds_to_seed = 0.3\n",
    "    length_to_seed = int(16000*seconds_to_seed - 1)\n",
    "    # [1, seed_len]; ensure <= available length\n",
    "    seed = audio[:1, : length_to_seed]\n",
    "\n",
    "    # Seconds to generate.\n",
    "    sec_to_generate = 1.7\n",
    "    n_new = int(16000 * sec_to_generate)\n",
    "\n",
    "    play_encoded_sample(seed.cpu())\n",
    "\n",
    "    display_waveform(seed.cpu(), \"Seed\")\n",
    "\n",
    "    audio_full = generate_continuation(\n",
    "        model.compiled_model,\n",
    "        seed_tokens=seed,\n",
    "        n_steps=n_new,\n",
    "        temperature=1.0,      # tweak 0.7–1.2\n",
    "        top_k=100,            # optional; try None or 50–200\n",
    "        device=device,\n",
    "    )\n",
    "    # audio_full_greedy = generate_greedy(model.base_model, seed, n_new, device=device)\n",
    "\n",
    "    play_encoded_sample(audio_full.cpu())\n",
    "    display_waveform(audio_full.cpu(), \"audio_full\")\n",
    "\n",
    "    # play_encoded_sample(audio_full_greedy.cpu())\n",
    "    # display_waveform(audio_full_greedy.cpu(), \"audio_full_greedy\")\n",
    "\n",
    "\n",
    "gen(0)\n",
    "gen(1)\n",
    "gen(2)\n",
    "gen(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c74889",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = model.get_receptive_field()\n",
    "\n",
    "# use the training loader for the overfit window\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "i = 0  # or pick the exact index you overfit on\n",
    "# Optionally pick a prefix length N to avoid pads; else use full length\n",
    "# example: 0.5s @16k if tokens==samples\n",
    "N = min(8000, x_batch.size(1))\n",
    "q = x_batch[i:i+1, :N].to(device)             # [1, N]\n",
    "y = y_batch[i:i+1, :N].to(device)             # [1, N]\n",
    "\n",
    "# q: [1, W] tokens of the SAME training window you overfit\n",
    "S = max(rf, (q.size(1)-1)//2)            # seed ≥ RF, inside the same window\n",
    "seq = q[:, :S].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range((q.size(1)-1) - S)):\n",
    "        ctx = seq[:, -rf:] if seq.size(1) > rf else seq\n",
    "        logits = model.base_model(ctx)[:, -1, :].float()  # cast logits to fp32\n",
    "        nxt = logits.argmax(-1, keepdim=True)  # greedy\n",
    "        seq = torch.cat([seq, nxt], 1)\n",
    "\n",
    "pred = seq[:, S:]                # [1, W-1-S]\n",
    "tgt = q[:, S+1:].to(device)\n",
    "fr_acc = (pred == tgt).float().mean().item()\n",
    "print(\"FR acc within-window:\", fr_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
